import nipype.interfaces.afni as afni
import nibabel as nib
import numpy as np
import os
import pathlib
import shutil
from os.path import join as opj
from os import chdir
from pathlib import Path
from os.path import exists
import time
import filecmp


# start over

chdir("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/")
shutil.rmtree('FT.results')



print("auto-generated by afni_proc.py, Tue Jan 31 15:45:04 2023")
print("(version 7.51, January 24, 2023)")
print("execution started: `date`")


# fonction to check that two brik file are similar
# convert from BRIK to NII and then compare
def compare2brik(brik1_path, brik2_path):
    if os.path.isdir('temp_Brik2Nifti_transformation_folder'):
        shutil.rmtree('temp_Brik2Nifti_transformation_folder')
    pathlib.Path("temp_Brik2Nifti_transformation_folder").mkdir(parents=True, exist_ok=True)
    a2n = afni.AFNItoNIFTI()
    a2n.inputs.in_file = brik1_path
    a2n.inputs.out_file =  'temp_Brik2Nifti_transformation_folder/nii1.nii'
    a2n.run() 
    print("transform BRIK to NII")
    time.sleep(1)
    a2n = afni.AFNItoNIFTI()
    a2n.inputs.in_file = brik2_path
    a2n.inputs.out_file =  'temp_Brik2Nifti_transformation_folder/nii2.nii'
    a2n.run()
    print("transform BRIK to NII")
    time.sleep(1)
    nii1_data = np.asanyarray(nib.load('temp_Brik2Nifti_transformation_folder/nii1.nii').get_fdata())
    nii2_data = np.asanyarray(nib.load('temp_Brik2Nifti_transformation_folder/nii2.nii').get_fdata())
    
    print(nii1_data.shape, " VERSUS ", nii2_data.shape)
    print('----#####-----')
    print(np.array_equal(nii1_data, nii2_data))
    print('----#####-----')
    shutil.rmtree('temp_Brik2Nifti_transformation_folder')
    time.sleep(2)

# careful, same text file might not be similar in terms of bite
def compare2text(txt1_path, txt2_path):
    # reading files
    print("****** CHECKING TEXT FILES ******")
    print(txt2_path)
    print("****** START ******")
    f1 = open(txt1_path, "r") 
    f2 = open(txt2_path, "r") 
    f1_data = f1.readlines()
    f2_data = f2.readlines()
    diff = 0
    try:
        print("file 1 :")
        for i in range(5):
            print(f1_data[i])
    except:
        print("Less than 5 lines")
    try:
        print("file 2 :")
        for i in range(5):
            print(f2_data[i])
    except:
        print("Less than 5 lines")
    # closing files
    f1.close()                                      
    f2.close()
    print('----#####-----')
    print(filecmp.cmp(txt1_path, txt2_path))
    print('----#####-----')
    print("****** END ******")
    time.sleep(2)



# =========================== auto block: setup ============================ # WORKING SMOOTHLY
# script setup

# assign sub and output directory name
sub = 'FT'
output_dir = 'FT.results'
# set list of runs
runs = [1, 2, 3]
path_afni_preprocessed_files = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/"

# create results and stimuli directories
pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)
pathlib.Path(opj(output_dir, 'stimuli')).mkdir(parents=True, exist_ok=True)



# copy stim files into stimulus directory
src1 = opj(sub, "AV1_vis.txt") 
src2 = opj(sub, "AV2_aud.txt")
dst = opj(output_dir, "stimuli")
shutil.copy(src1, dst)
shutil.copy(src2, dst)

# copy anatomy to results dir
src1 = "FT/FT_anat+orig.BRIK"
src2 = "FT/FT_anat+orig.HEAD"
# copy template to results dir (for QC)
src3 = "/home/jlefortb/abin/TT_N27+tlrc.HEAD"
src4 = "/home/jlefortb/abin/TT_N27+tlrc.BRIK.gz"
dst = "FT.results/"
shutil.copy(src1, dst)
shutil.copy(src2, dst)
shutil.copy(src3, dst)
shutil.copy(src4, dst)

# check outputs
brik1_path_nipype = '/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/FT_anat+orig.BRIK'
brik2_path_afni = '/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/FT_anat+orig.BRIK'
compare2brik(brik1_path_nipype, brik2_path_afni) # True
compare2brik(src4, "FT.results/TT_N27+tlrc.BRIK.gz") # True

# ============================ auto block: tcat ============================ # ERROR
# apply 3dTcat to copy input dsets to results dir,
# while removing the first 2 TRs
for run in runs:
    tcsb = afni.TCatSubBrick() # allow sub-brick selection 
    tcsb.inputs.in_files = [('FT/FT_epi_r{}+orig.BRIK'.format(run), "'[2..$]'")]
    tcsb.inputs.out_file = opj(output_dir, "pb00.{}.r0{}.tcat".format(sub, run))
    tcsb.cmdline
    print("cmd line is: ", tcsb.cmdline)
    print("cmd line is: ", tcsb.cmdline)
    # should be
    # 3dTcat -prefix $output_dir/pb00.$subj.r01.tcat FT/FT_epi_r1+orig'[2..$]'
    res = tcsb.run()
    # compare with 'tutorial using afni' results
    compare2brik(opj(path_afni_preprocessed_files,"pb00.FT.r0{}.tcat+orig.BRIK".format(run)), "FT.results/pb00.FT.r0{}.tcat+orig.BRIK".format(run)) # TRUE

# and make note of repetitions (TRs) per run
tr_counts = [150, 150, 150]

# -------------------------------------------------------
# enter the results directory (can begin processing data)

chdir(output_dir)




"""missing
# ---------------------------------------------------------
# QC: compute correlations with spherical ~averages
@radial_correlate -nfirst 0 -polort 3 -do_clean yes \
                  -rdir radcor.pb00.tcat            \
                  pb00.$subj.r*.tcat+orig.HEAD

# ---------------------------------------------------------
# QC: look for columns of high variance
find_variance_lines.tcsh -polort 3 -nerode 2        \
       -rdir vlines.pb00.tcat                       \
       pb00.$subj.r*.tcat+orig.HEAD |& tee out.vlines.pb00.tcat.txt
"""
# not working in nipype thus copying output directories from afni tutorial with afni

src = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/radcor.pb00.tcat"
dst = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/radcor.pb00.tcat"
shutil.copytree(src, dst)
src = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vlines.pb00.tcat"
dst = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/vlines.pb00.tcat"
shutil.copytree(src, dst)




# ========================== auto block: outcount ========================== # ERROR
# QC: compute outlier fraction for each volume

Path('out.pre_ss_warn.txt').touch()

# ERROR : source code string cannot contain null bytes
%run /home/jlefortb/abin/1deval -a /home/jlefortb/reproduction_afni_tutorial/afnit_tuto_w_afni/FT.results/outcount.r01.1D -expr "1-step(a-0.05)" > rm.out.cen.r01.1D

for run in runs:
    # Calculates number of ‘outliers’ at each time point of a a 3D+time dataset.
    toutcount = afni.OutlierCount(automask=True, fraction=True, polort=3, legendre=True)
    toutcount.inputs.in_file = 'pb00.{}.r0{}.tcat+orig.BRIK'.format(sub, run)
    toutcount.inputs.out_file = 'outcount.r0{}.1D'.format(run)
    toutcount.cmdline
    print("Cmd line is : ", toutcount.cmdline)
    # should be
    # 3dToutcount -automask -fraction -polort 3 -legendre                     \
    #           pb00.$subj.r$run.tcat+orig > outcount.r$run.1D
    res = toutcount.run()

    # check output
    compare2text(opj(path_afni_preprocessed_files, 'outcount.r0{}.1D'.format(run)), 'outcount.r0{}.1D'.format(run))


    # censor outlier TRs per run, ignoring the first 0 TRs
    # - censor when more than 0.05 of automask voxels are outliers
    # - step() defines which TRs to remove via censoring
    eval = afni.Eval()# Evaluates an expression that may include columns of data from one or more text files
    # Evaluates an expression that may include columns of data from one or more text files.
    eval.inputs.in_file_a = 'outcount.r0{}.1D'.format(run)
    eval.inputs.expr = "1-step(a-0.05)"
    eval.inputs.out_file =  'rm.out.cen.r0{}.1D'.format(run)
    # eval needs an existing file name
    Path('rm.out.cen.r0{}.1D'.format(run)).touch()
    eval.cmdline
    print("Cmd line is : ", eval.cmdline)
    # should be:
    # 1deval -a outcount.r$run.1D -expr "1-step(a-0.05)" > rm.out.cen.r$run.1D
    res = eval.run() 

    # outliers at TR 0 might suggest pre-steady state TRs
    eval = afni.Eval()# Evaluates an expression that may include columns of data from one or more text files
    eval.inputs.in_file_a = 'outcount.r0{}.1D'.format(run)
    eval.inputs.single_idx = 0
    eval.inputs.expr = 'step(a-0.4)'
    eval.inputs.out_file = "out.pre_ss_warn.txt"
    eval.cmdline
    # should be:
    # if (`1deval -a outcount.r$run.1D"{0}" -expr "step(a-0.4)"`):
    #   print("** TR #0 outliers: possible pre-steady state TRs in run {}".format(run), file=f)
    eval.run()

    compare2text(opj(path_afni_preprocessed_files, "out.pre_ss_warn.txt"), "out.pre_ss_warn.txt")


# catenate outlier counts into a single time series
filenames = ['outcount.r01.1D', 'outcount.r02.1D', 'outcount.r03.1D']
with open('outcount_rall.1D', 'w') as outfile:
    for fname in filenames:
        with open(fname) as infile:
            outfile.write(infile.read()+'\n')

compare2text(opj(path_afni_preprocessed_files, 'outcount_rall.1D'), 'outcount_rall.1D')

# catenate outlier censor files into a single time series
filenames = ['rm.out.cen.r01.1D', 'rm.out.cen.r02.1D', 'rm.out.cen.r03.1D']
# cat rm.out.cen.r*.1D > outcount_${subj}_censor.1D
with open('outcount_{}_censor.1D'.format(sub), 'w') as outfile:
    for fname in filenames:
        with open(fname) as infile:
            outfile.write(infile.read()+'\n')
compare2text(opj(path_afni_preprocessed_files, 'outcount_{}_censor.1D'.format(sub)), 'outcount_{}_censor.1D'.format(sub))


# get run number and TR index for minimum outlier volume
with open('/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/outcount_rall.1D') as f:
    lines = f.readlines()
lines = [float(line) for line in lines]
minindex = lines.index(min(lines))

# get TR and run number
if minindex <= 150:
    minoutr = minindex
    run_nb = 1
elif 150 < minindex <= 300:
    minoutr = minindex - 150
    run_nb = 2
else:
    minoutr = minindex - 300
    run_nb = 3
# save run and TR indices for extraction of vr_base_min_outlier
with open("out.min_outlier.txt", "a") as f:
  print("min outlier: run 0{}, TR {}".format(run_nb, minoutr), file=f)
compare2text(opj(path_afni_preprocessed_files, "out.min_outlier.txt"), "out.min_outlier.txt")


# ================================= tshift ================================= # WORKING SMOOTHLY
# time shift data so all slice timing is the same 
for run in runs:
    tshift = afni.TShift() # Shifts voxel time series from input so that seperate slices are aligned to the same temporal origin.
    tshift.inputs.args = '-quintic'
    tshift.inputs.in_file = 'pb00.{}.r0{}.tcat+orig.BRIK'.format(sub, run)
    tshift.inputs.tzero = 0
    tshift.inputs.out_file = 'pb01.{}.r0{}.tshift+orig.BRIK'.format(sub, run)
    tshift.cmdline
    # should be 3dTshift -tzero 0 -quintic -prefix pb01.$subj.r$run.tshift \
    #         pb00.$subj.r$run.tcat+orig
    res = tshift.run()

# check with 'tutorial using afni' results
for run in runs:
    compare2brik("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/pb01.FT.r0{}.tshift+orig.BRIK".format(run), "pb01.FT.r0{}.tshift+orig.BRIK".format(run)) # TRUE

# --------------------------------
# extract volreg registration base
bucket = afni.Bucket() # Concatenate sub-bricks from input datasets into one big ‘bucket’ dataset.
bucket.inputs.in_file = [('pb01.{}.r0{}.tshift+orig.BRIK'.format(sub, run_nb),"[{}]".format(minoutr))]
bucket.inputs.out_file = 'vr_base_min_outlier'
bucket.cmdline
# should be
# 3dbucket -prefix vr_base_min_outlier                           \
#     pb01.$subj.r$minoutrun.tshift+orig"[$minouttr]"
res = bucket.run()  # doctest: +SKIP

compare2brik("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vr_base_min_outlier+orig.BRIK", "vr_base_min_outlier+orig.BRIK") # TRUE



# ================================= align ================================== # ERROR
# for e2a: compute anat alignment transformation to EPI registration base
# (new anat will be intermediate, stripped, FT_anat_ns+orig)
# run (localized) uniformity correction on EPI base



"""missing
# should be :
# 3dLocalUnifize -input vr_base_min_outlier+orig -prefix \
#     vr_base_min_outlier_unif
"""
# 3dUnifize in nipype but no 3dLocalUnifize
# thus copying output directories from afni tutorial with afni
import shutil
src = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vr_base_min_outlier_unif+orig.BRIK"
dst = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/vr_base_min_outlier_unif+orig.BRIK"
shutil.copyfile(src, dst)
src = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vr_base_min_outlier_unif+orig.HEAD"
dst = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/vr_base_min_outlier_unif+orig.HEAD"
shutil.copyfile(src, dst)
compare2brik("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vr_base_min_outlier_unif+orig.BRIK", "vr_base_min_outlier_unif+orig.BRIK") # TRUE

###################
# ERROR HERE
# AttributeError: 'dict' object has no attribute 'skullstrip'
# prg ran with AFNI => no pb
# thus pb comes from nipype
###################

# computes the alignment between two datasets, typically an EPI and an anatomical 
# structural dataset, and applies the resulting transformation to one or the other 
# to bring them into alignment

"""missing
al_ea = afni.AlignEpiAnatPy()
al_ea.inputs.anat2epi = True
al_ea.inputs.anat = "FT_anat+orig.BRIK"
al_ea.inputs.save_skullstrip = True
al_ea.inputs.suffix = "_al_junk"
al_ea.inputs.in_file = "vr_base_min_outlier_unif+orig.BRIK"
al_ea.inputs.epi_base = 0
al_ea.inputs.epi_strip = '3dAutomask'
al_ea.inputs.args = "-cost lpc+ZZ -giant_move -check_flip"
al_ea.inputs.volreg = 'off'
al_ea.inputs.tshift = 'off'
al_ea.cmdline
# should be
# align_epi_anat.py -anat2epi -anat FT_anat+orig         \
#        -save_skullstrip -suffix _al_junk               \
#        -epi vr_base_min_outlier_unif+orig -epi_base 0  \
#        -epi_strip 3dAutomask                           \
#        -cost lpc+ZZ -giant_move -check_flip            \
#        -volreg off -tshift off
res = al_ea.run()  # last for 5 minutes
"""
# running afni prog directly instead
%run /home/jlefortb/abin/align_epi_anat.py -anat2epi -anat FT_anat+orig         \
       -save_skullstrip -suffix _al_junk               \
       -epi vr_base_min_outlier_unif+orig -epi_base 0  \
       -epi_strip 3dAutomask                            \
       -cost lpc+ZZ -giant_move -check_flip            \
       -volreg off -tshift off


paths_to_test = ["FT_anat_unflipped+orig.BRIK", 
            "vr_base_min_outlier_unif_ts_ns+orig.BRIK", 
            "vr_base_min_outlier_unif_ts_ns_wt+orig.BRIK", 
            "FT_anat_unflipped_ns_al_junk_wtal+orig.BRIK",
            "FT_anat_al_junk+orig.BRIK",
            "FT_anat_flip_al_junk+orig.BRIK"] 
for path_to_test in paths_to_test:
    print(path_to_test)
    compare2brik("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/{}".format(path_to_test), path_to_test) # TRUE



# ================================== tlrc ==================================  # ERROR
# warp anatomy to standard space
autoTLRC = afni.AutoTLRC() # A minmal wrapper for the AutoTLRC script
autoTLRC.inputs.in_file = 'FT_anat_ns+orig.BRIK.gz'
autoTLRC.inputs.no_ss = True
autoTLRC.inputs.base = "TT_N27+tlrc"
autoTLRC.cmdline
# should be
# @auto_tlrc -base TT_N27+tlrc -input FT_anat_ns+orig -no_ss
res = autoTLRC.run()
paths_to_test = ["FT_anat_ns+tlrc.BRIK"] 
for path_to_test in paths_to_test:
    print(path_to_test)
    compare2brik("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/{}".format(path_to_test), path_to_test) # TRUE
# ERROR IS DISPLAYED but effectivelly worked (the error is due to the cleaning process afterwards)


# # running afni prog directly instead
# !tcsh /home/jlefortb/abin/@auto_tlrc -base TT_N27+tlrc -input FT_anat_ns+orig -no_ss

# store forward transformation matrix in a text file
cmv = afni.CatMatvec() # Catenates 3D rotation+shift matrix+vector transformations.
cmv.inputs.in_file = [('FT_anat_ns+tlrc::WARP_DATA','I')]
cmv.inputs.out_file = 'warp.anat.Xat.1D'
cmv.cmdline
# should be
# cat_matvec FT_anat_ns+tlrc::WARP_DATA -I > warp.anat.Xat.1D
res = cmv.run()

compare2text(opj(path_afni_preprocessed_files, 'warp.anat.Xat.1D'), 'warp.anat.Xat.1D')


# ================================= volreg =================================
# align each dset to base volume, to anat, warp to tlrc space

# verify that we have a +tlrc warp dataset
assert exists("FT_anat_ns+tlrc.HEAD"), "** missing +tlrc warp dataset: FT_anat_ns+tlrc.HEAD"



# register and warp
for run in runs:
    # register each volume to the base image
    volreg = afni.Volreg() # Register input volumes to a base volume using AFNI 3dvolreg command
    volreg.inputs.in_file = 'pb01.{}.r0{}.tshift+orig.BRIK'.format(sub, run)
    volreg.inputs.interp = 'cubic'
    volreg.inputs.verbose = True
    volreg.inputs.zpad = 1
    volreg.inputs.basefile = 'vr_base_min_outlier+orig.BRIK'
    volreg.inputs.out_file = 'rm.epi.volreg.r0{}+orig.BRIK'.format(run)
    volreg.inputs.oned_file = 'dfile.r0{}.1D'.format(run)
    volreg.inputs.oned_matrix_save = 'mat.r0{}.vr.aff12.1D'.format(run)
    volreg.cmdline
    # should be 
    # 3dvolreg -verbose -zpad 1 -base vr_base_min_outlier+orig    \
    #      -1Dfile dfile.r$run.1D -prefix rm.epi.volreg.r$run \
    #      -cubic                                             \
    #      -1Dmatrix_save mat.r$run.vr.aff12.1D               \
    #      pb01.$subj.r$run.tshift+orig
    res = volreg.run()

    # Above is working, output checked with original AFNI code 

    # create an all-1 dataset to mask the extents of the warp
    calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
    calc.inputs.in_file_a = 'pb01.{}.r0{}.tshift+orig.BRIK'.format(sub, run)
    calc.inputs.expr = '1'
    calc.inputs.out_file = 'rm.epi.all1+orig.BRIK.gz'
    calc.inputs.overwrite = True
    calc.cmdline
    # should be
    # 3dcalc -overwrite -a pb01.$subj.r$run.tshift+orig -expr 1   \
    #        -prefix rm.epi.all1
    res = calc.run()
    
    # Above is working, output checked with original AFNI code 

    # catenate volreg/epi2anat/tlrc xforms
    cmv = afni.CatMatvec() # Catenates 3D rotation+shift matrix+vector transformations.
    cmv.inputs.in_file = [('FT_anat_ns+tlrc::WARP_DATA','I'), ('FT_anat_al_junk_mat.aff12.1D','I'), ("mat.r0{}.vr.aff12.1D".format(run), '')]
    cmv.inputs.out_file = 'mat.r0{}.warp.aff12.1D'.format(run)
    cmv.inputs.oneline = True
    cmv.cmdline
    # should be
    # cat_matvec -ONELINE                                         \
    #            FT_anat_ns+tlrc::WARP_DATA -I                    \
    #            FT_anat_al_junk_mat.aff12.1D -I                  \
    #            mat.r$run.vr.aff12.1D > mat.r$run.warp.aff12.1D
    res = cmv.run()

    # Above is working, output checked with original AFNI code 


    # apply catenated xform: volreg/epi2anat/tlrc
    allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
    allineate.inputs.args = '-mast_dxyz 2.5'
    allineate.inputs.reference = 'FT_anat_ns+tlrc.BRIK'
    allineate.inputs.in_file = 'pb01.{}.r0{}.tshift+orig.BRIK'.format(sub, run)
    allineate.inputs.in_matrix = 'mat.r0{}.warp.aff12.1D'.format(run)
    allineate.inputs.out_file = 'rm.epi.nomask.r0{}+tlrc.BRIK'.format(run)
    allineate.cmdline
    # should be
    # 3dAllineate -base FT_anat_ns+tlrc                           \
    #             -input pb01.$subj.r$run.tshift+orig             \
    #             -1Dmatrix_apply mat.r$run.warp.aff12.1D         \
    #             -mast_dxyz 2.5                                  \
    #             -prefix rm.epi.nomask.r$run
    res = allineate.run()
    
    # Above is working, output checked with original AFNI code 

    # warp the all-1 dataset for extents masking 
    allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
    allineate.inputs.args = '-final NN -mast_dxyz 2.5'
    allineate.inputs.quiet = True
    allineate.inputs.reference = 'FT_anat_ns+tlrc.BRIK'
    allineate.inputs.in_file = 'rm.epi.all1+orig.BRIK.gz'
    allineate.inputs.in_matrix = 'mat.r0{}.warp.aff12.1D'.format(run)
    allineate.inputs.out_file = 'rm.epi.1.r0{}+tlrc.BRIK.gz'.format(run)
    allineate.cmdline
    # should be
    # 3dAllineate -base FT_anat_ns+tlrc                           \
    #             -input rm.epi.all1+orig                         \
    #             -1Dmatrix_apply mat.r$run.warp.aff12.1D         \
    #             -mast_dxyz 2.5 -final NN -quiet                 \
    #             -prefix rm.epi.1.r$run
    res = allineate.run()

    # make an extents intersection mask of this run
    tstat = afni.TStat() # Compute voxel-wise statistics
    tstat.inputs.in_file = 'rm.epi.1.r0{}+tlrc.BRIK.gz'.format(run)
    tstat.inputs.args = '-min'
    tstat.inputs.out_file = 'rm.epi.min.r0{}+tlrc.BRIK.gz'.format(run)
    tstat.cmdline
    # should be
    # 3dTstat -min -prefix rm.epi.min.r$run rm.epi.1.r$run+tlrc
    res = tstat.run()



# make a single file of registration params
# catenate outlier counts into a single time series
filenames = ['dfile.r01.1D', 'dfile.r02.1D', 'dfile.r03.1D']
with open('dfile_rall.1D', 'w') as outfile:
    for fname in filenames:
        with open(fname) as infile:
            outfile.write(infile.read() +'\n')
# cat dfile.r*.1D > dfile_rall.1D
compare2text(opj(path_afni_preprocessed_files, 'dfile_rall.1D'), 'dfile_rall.1D')


# WORK UNTIL HERE
##################
# CURRENTLY HERE #
##################

from glob import glob
# ----------------------------------------
# create the extents mask: mask_epi_extents+tlrc
# (this is a mask of voxels that have valid data at every TR)
means = afni.Means() # Takes the voxel-by-voxel mean of all input datasets using 3dMean
means.inputs.datum = 'short'
means.inputs.in_file_a = 'rm.epi.min.r01+tlrc.HEAD'
means.inputs.in_file_b = 'rm.epi.min.r02+tlrc.HEAD'
means.inputs.out_file =  'rm_temp.epi.mean+tlrc.BRIK.gz'
means.cmdline
res = means.run() 

means = afni.Means() # Takes the voxel-by-voxel mean of all input datasets using 3dMean
means.inputs.datum = 'short'
means.inputs.in_file_a = 'rm_temp.epi.mean+tlrc.HEAD'
means.inputs.in_file_b = 'rm.epi.min.r03+tlrc.HEAD'
means.inputs.out_file =  'rm.epi.mean+tlrc.BRIK.gz'
means.cmdline
res = means.run() 

# should be
!3dMean -datum short -prefix rmtest.epi.mean rm.epi.min.r*.HEAD 


calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
calc.inputs.in_file_a = 'rm.epi.mean+tlrc'
calc.inputs.expr='step(a-0.999)'
calc.inputs.out_file =  'mask_epi_extents'
calc.cmdline  # doctest: +ELLIPSIS
# should be
# 3dcalc -a rm.epi.mean+tlrc -expr 'step(a-0.999)' -prefix mask_epi_extents
res = calc.run()  # doctest: +SKIP


# and apply the extents mask to the EPI data 
# (delete any time series with missing data)
for run in runs:
    calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
    calc.inputs.in_file_a = 'rm.epi.nomask.r{}+tlrc'.format(run)
    calc.inputs.in_file_b = 'mask_epi_extents+tlrc'
    calc.inputs.expr='a*b'
    calc.inputs.out_file =  'pb02.{}.r{}.volreg'.format(sub, run)
    calc.cmdline  # doctest: +ELLIPSIS
    # should be
    # 3dcalc -a rm.epi.nomask.r$run+tlrc -b mask_epi_extents+tlrc \
    #        -expr 'a*b' -prefix pb02.$subj.r$run.volreg
    res = calc.run()  # doctest: +SKIP



# warp the volreg base EPI dataset to make a final version
cmv = afni.Bucket() # Concatenate sub-bricks from input datasets into one big ‘bucket’ dataset.
cmv.inputs.in_file = [('FT_anat_ns+tlrc::WARP_DATA','I'), ('FT_anat_al_junk_mat.aff12.1D','I')]
cmv.inputs.out_file = 'mat.basewarp.aff12.1D'
cmv.inputs.oneline = True
cmv.cmdline
# should be
# cat_matvec -ONELINE                                             \
#            FT_anat_ns+tlrc::WARP_DATA -I                        \
#            FT_anat_al_junk_mat.aff12.1D -I  > mat.basewarp.aff12.1D
res = cmv.run()


# warp the all-1 dataset for extents masking 
allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
allineate.args = '-final NN -mast_dxyz 2.5'
allineate.quiet = True
allineate.reference = 'FT_anat_ns+tlrc'
allineate.inputs.in_file = 'vr_base_min_outlier+orig'
allineate.inputs.in_matrix = 'mat.basewarp.aff12.1D'
allineate.inputs.out_file = 'final_epi_vr_base_min_outlier'
allineate.cmdline
# should be
# 3dAllineate -base FT_anat_ns+tlrc                               \
#             -input vr_base_min_outlier+orig                     \
#             -1Dmatrix_apply mat.basewarp.aff12.1D               \
#             -mast_dxyz 2.5                                      \
#             -prefix final_epi_vr_base_min_outlier
res = allineate.run()



# create an anat_final dataset, aligned with stats
copy3d=  afni.Copy() # Copies an image of one type to an image of the same or different type
copy3d.inputs.in_file = "FT_anat_ns+tlrc"
copy3d.inputs.out_file = "anat_final.{}".format(sub)
copy3d.cmdline
# should be 
# 3dcopy FT_anat_ns+tlrc anat_final.$subj
res = copy3d.run()

# record final registration costs
allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
allineate.quiet = True
allineate.args = "-allcostX"
allineate.reference = 'final_epi_vr_base_min_outlier+tlrc'
allineate.inputs.in_file = 'anat_final.{}+tlrc'.format(sub)
allineate.inputs.allcostx = 'out.allcostX.txt'
allineate.cmdline
# should be
# 3dAllineate -base final_epi_vr_base_min_outlier+tlrc -allcostX  \
#             -input anat_final.$subj+tlrc |& tee out.allcostX.txt
res = allineate.run()





# --------------------------------------
# create a TSNR dataset, just from run 1
tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_file = 'rm.signal.vreg.r01'
tstat.inputs.args = '-mean'
tstat.inputs.out_file = 'pb02.{}.r01.volreg+tlrc'.format(sub)
tstat.cmdline
# should be
# 3dTstat -mean -prefix rm.signal.vreg.r01 pb02.$subj.r01.volreg+tlrc
res = tstat.run()

detrend = afni.Detrend()
detrend.inputs.in_file = 'pb02.{}.r01.volreg+tlrc'.format(sub)
detrend.inputs.args = '-polort 3 -overwrite'
detrend.inputs.outputtype = 'AFNI'
detrend.inputs.out_file = 'rm.noise.det'
detrend.cmdline
# should be
# 3dDetrend -polort 3 -prefix rm.noise.det -overwrite pb02.$subj.r01.volreg+tlrc
res = detrend.run()  # doctest: +SKIP


tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_file = 'rm.noise.vreg.r01'
tstat.inputs.args = '-stdev'
tstat.inputs.out_file = 'rm.noise.det+tlrc'
tstat.cmdline
# should be
# 3dTstat -stdev -prefix rm.noise.vreg.r01 rm.noise.det+tlrc
res = tstat.run()


calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
calc.inputs.in_file_a = 'rm.signal.vreg.r01+tlrc'
calc.inputs.in_file_b = 'rm.noise.vreg.r01+tlrc'
calc.inputs.in_file_c = 'mask_epi_extents+tlrc'
calc.inputs.expr='c*a/b'
calc.inputs.out_file =  'TSNR.vreg.r01.{}'.format(sub)
calc.cmdline
# should be
# 3dcalc -a rm.signal.vreg.r01+tlrc                               \
#        -b rm.noise.vreg.r01+tlrc                                \
#        -c mask_epi_extents+tlrc                                 \
#        -expr 'c*a/b' -prefix TSNR.vreg.r01.$subj
res = calc.run() 



# -----------------------------------------
# warp anat follower datasets (affine)
# warp follower dataset FT_anat+orig
allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
allineate.quiet = True
allineate.inputs.in_file = 'FT_anat+orig'
allineate.inputs.master = 'anat_final.{}}+tlrc'.format(sub)
allineate.inputs.final_interpolation = 'wsinc5'
allineate.inputs.in_matrix = 'warp.anat.Xat.1D'
allineate.inputs.out_file = 'anat_w_skull_warped'
allineate.cmdline
# should be
# 3dAllineate -source FT_anat+orig                                \
#             -master anat_final.$subj+tlrc                       \
#             -final wsinc5 -1Dmatrix_apply warp.anat.Xat.1D      \
#             -prefix anat_w_skull_warped
res = allineate.run()

# ---------------------------------------------------------

"""missing
# QC: compute correlations with spherical ~averages
@radial_correlate -nfirst 0 -polort 3 -do_clean yes             \
                  -rdir radcor.pb02.volreg                      \
                  pb02.$subj.r*.volreg+tlrc.HEAD
"""


# ================================== blur ==================================
# blur each volume of each run
for run in runs:
    merge = afni.Merge() # Merge or edit volumes using AFNI 3dmerge command
    merge.inputs.in_files = 'pb02.{}.r{}.volreg+tlrc'.format(sub, run)
    merge.inputs.blurfwhm = 4
    merge.inputs.doall = True
    merge.inputs.out_file = 'pb03.{}.r{}.blur'.format(sub, run)
    merge.cmdline
    # should be:
    # 3dmerge -1blur_fwhm 4.0 -doall -prefix pb03.$subj.r$run.blur \
    #         pb02.$subj.r$run.volreg+tlrc
    res = merge.run() 



# ================================== mask ==================================
# create 'full_mask' dataset (union mask)
for run in runs:
    automask = afni.Automask() # Create a brain-only mask of the image using AFNI 3dAutomask command
    automask.inputs.in_file = 'pb03.$subj.r$run.blur+tlrc'
    automask.inputs.out_file = "rm.mask_r{}".format(run)
    automask.cmdline  # doctest: +ELLIPSIS
    # should be:
    # 3dAutomask -prefix rm.mask_r$run pb03.$subj.r$run.blur+tlrc
    res = automask.run()  # doctest: +SKIP

# create union of inputs, output type is byte
masktool = afni.MaskTool() # for combining/dilating/eroding/filling masks
masktool.inputs.in_file = 'rm.mask_r*+tlrc.HEAD'
masktool.inputs.out_file = "full_mask.{}".format(sub)
masktool.inputs.union = True
masktool.cmdline
# should be:
# 3dmask_tool -inputs rm.mask_r*+tlrc.HEAD -union -prefix full_mask.$subj
res = automask.run() 


# ---- create subject anatomy mask, mask_anat.$subj+tlrc ----
#      (resampled from tlrc anat)
resample = afni.Resample() # Resample or reorient an image using AFNI 3dresample command
resample.inputs.in_file = 'FT_anat_ns+tlrc'
resample.inputs.master = 'full_mask.{}+tlrc'.format(sub)
resample.inputs.out_file = 'rm.resam.anat'
resample.cmdline
# Should be:
# 3dresample -master full_mask.$subj+tlrc -input FT_anat_ns+tlrc        \
#            -prefix rm.resam.anat
res = resample.run()  # doctest: +SKIP


# convert to binary anat mask; fill gaps and holes
masktool = afni.MaskTool() # for combining/dilating/eroding/filling masks
masktool.inputs.in_file = 'rm.resam.anat+tlrc'
masktool.inputs.dilate_results = '5 -5'
masktool.inputs.fill_holes = True
masktool.inputs.out_file = "mask_anat.{}".format(sub)
masktool.cmdline
# should be:
# 3dmask_tool -dilate_input 5 -5 -fill_holes -input rm.resam.anat+tlrc  \
#             -prefix mask_anat.$subj
res = automask.run() 

# compute tighter EPI mask by intersecting with anat mask
masktool = afni.MaskTool() # for combining/dilating/eroding/filling masks
masktool.inputs.in_file = 'full_mask.{}+tlrc'.format(sub)
masktool.inputs.out_file = "mask_epi_anat.{}".format(sub)
masktool.inputs.inter = True
masktool.cmdline
# should be:
# 3dmask_tool -input full_mask.$subj+tlrc mask_anat.$subj+tlrc          \
#             -inter -prefix mask_epi_anat.$subj
res = automask.run() 


# compute overlaps between anat and EPI masks
aboverlap = afni.ABoverlap()
aboverlap.inputs.in_file_a = 'full_mask.{}+tlrc'.format(sub)
aboverlap.inputs.in_file_b = 'mask_anat.{}+tlrc'.format(sub)
aboverlap.inputs.no_automask = True
aboverlap.inputs.out_file =  'out.mask_ae_overlap.txt'
aboverlap.cmdline
# should be
# 3dABoverlap -no_automask full_mask.$subj+tlrc mask_anat.$subj+tlrc    \
#             |& tee out.mask_ae_overlap.txt
res = aboverlap.run() 


# note Dice coefficient of masks, as well
dot = afni.Dot() # Correlation coefficient between sub-brick pairs. All datasets in in_files list will be concatenated. 
dot.inputs.in_files = ['full_mask.{}+tlrc'.format(sub), 'mask_anat.{}+tlrc'.format(sub)]
dot.inputs.dodice = True
dot.inputs.out_file = 'out.mask_ae_dice.txt'
dot.cmdline
# should be
# 3ddot -dodice full_mask.$subj+tlrc mask_anat.$subj+tlrc               \
#       |& tee out.mask_ae_dice.txt
res = copy3d.run()  


# ---- create group anatomy mask, mask_group+tlrc ----
#      (resampled from tlrc base anat, TT_N27+tlrc)
resample = afni.Resample() # Resample or reorient an image using AFNI 3dresample command
resample.inputs.in_file = '/home/jlefortb/abin/TT_N27+tlrc'
resample.inputs.master = 'full_mask.{}+tlrc'.format(sub)
resample.inputs.out_file = './rm.resam.group'
resample.cmdline
# Should be:
# 3dresample -master full_mask.$subj+tlrc -prefix ./rm.resam.group      \
#            -input /home/jlefortb/abin/TT_N27+tlrc
res = resample.run()  # doctest: +SKIP


# convert to binary group mask; fill gaps and holes
masktool = afni.MaskTool() # for combining/dilating/eroding/filling masks
masktool.inputs.in_file = 'rm.resam.group+tlrc'
masktool.inputs.out_file = 'mask_group'
masktool.inputs.dilate_results = "5 -5"
masktool.inputs.fill_holes = True
masktool.cmdline
# should be:
# 3dmask_tool -dilate_input 5 -5 -fill_holes -input rm.resam.group+tlrc \
#             -prefix mask_group
res = automask.run() 

# note Dice coefficient of anat and template masks
dot = afni.Dot() # Correlation coefficient between sub-brick pairs. All datasets in in_files list will be concatenated. 
dot.inputs.in_files = ['mask_anat.{}+tlrc'.format(sub), 'mask_group+tlrc']
dot.inputs.dodice = True
dot.inputs.out_file = 'out.mask_at_dice.txt'
dot.cmdline
# should be
# 3ddot -dodice mask_anat.$subj+tlrc mask_group+tlrc                    \
#       |& tee out.mask_at_dice.txt
res = copy3d.run() 


# ================================= scale ==================================
# scale each voxel time series to have a mean of 100
# (be sure no negatives creep in)
# (subject to a range of [0,200])
for run in runs:
    tstat = afni.TStat() # Compute voxel-wise statistics
    tstat.inputs.in_file = 'rpb03.{}.r{}.blur+tlrc'.format(sub, run)
    tstat.inputs.out_file = 'rm.mean_r{}'.format(run)
    tstat.cmdline
    # should be:
    # 3dTstat -prefix rm.mean_r$run pb03.$subj.r$run.blur+tlrc
    res = tstat.run()

    calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
    calc.inputs.in_file_a = 'pb03.{}.r{}.blur+tlrc'.format(sub, run)
    calc.inputs.in_file_b = 'rm.mean_r{}+tlrc'.format(run)
    calc.inputs.in_file_c = 'mask_epi_extents+tlrc '
    calc.inputs.expr = 'c * min(200, a/b*100)*step(a)*step(b)'
    calc.inputs.out_file = 'pb04.{}.r{}.scale'.format(sub, run)
    calc.cmdline
    # should be:
    # 3dcalc -a pb03.$subj.r$run.blur+tlrc -b rm.mean_r$run+tlrc \
    #        -c mask_epi_extents+tlrc                            \
    #        -expr 'c * min(200, a/b*100)*step(a)*step(b)'       \
    #        -prefix pb04.$subj.r$run.scale
    res = calc.run()


# ================================ regress =================================

# compute de-meaned motion parameters (for use in regression)
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets. 
odt.inputs.in_file = 'dfile_rall.1D'
odt.inputs.set_nruns = 3
odt.inputs.demean = True
odt.inputs.derivative = True
odt.inputs.out_file = 'motion_dmean.1D'
odt.cmdline
# should be:
# 1d_tool.py -infile dfile_rall.1D -set_nruns 3                            \
#            -demean -write motion_demean.1D
res = odt.run() 


# compute motion parameter derivatives (just to have)
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets. 
odt.inputs.in_file = 'dfile_rall.1D'
odt.inputs.set_nruns = 3
odt.inputs.demean = True
odt.inputs.out_file = 'motion_deriv.1D'
odt.cmdline
# should be:
# 1d_tool.py -infile dfile_rall.1D -set_nruns 3                            \
#            -derivative -demean -write motion_deriv.1D
res = odt.run() 

# convert motion parameters for per-run regression
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets. 
odt.inputs.in_file = 'motion_demean.1D'
odt.inputs.set_nruns = 3
odt.inputs.split_into_pad_runs = "mot_demean"
odt.inputs.out_file = 'motion_deriv.1D'
odt.cmdline
# should be:
# 1d_tool.py -infile motion_demean.1D -set_nruns 3                         \
#            -split_into_pad_runs mot_demean
res = odt.run() 


# create censor file motion_${subj}_censor.1D, for censoring motion
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets. 
odt.inputs.in_file = 'dfile_rall.1D'
odt.inputs.set_nruns = 3
odt.inputs.censor_motion = (0.3, 'motion_{}'.format(sub))
odt.inputs.censor_prev_TR = True
odt.inputs.show_censor_count = true
odt.cmdline
# should be:
# 1d_tool.py -infile dfile_rall.1D -set_nruns 3                            \
#     -show_censor_count -censor_prev_TR                                   \
#     -censor_motion 0.3 motion_${subj}
res = odt.run() 


# combine multiple censor files
eval = afni.Eval()# Evaluates an expression that may include columns of data from one or more text files
eval.inputs.in_file_a = 'motion_{}_censor.1D'.format(sub)
eval.inputs.in_file_b = 'outcount_{}_censor.1D'.format(sub)
eval.inputs.expr = 'a*b'
eval.inputs.out_file =  'censor_{}_combined_2.1D'.format(sub)
eval.cmdline
# should be:
# 1deval -a motion_${subj}_censor.1D -b outcount_${subj}_censor.1D         \
#        -expr "a*b" > censor_${subj}_combined_2.1D
res = eval.run() 

"""missing

# note TRs that were not censored
set ktrs = `1d_tool.py -infile censor_${subj}_combined_2.1D              \
                       -show_trs_uncensored encoded`

"""

# ------------------------------
# run the regression analysis
deconvolve = afni.Deconvolve() # Performs OLS regression given a 4D neuroimage file and stimulus timings
deconvolve.inputs.in_files = ['pb04.{}.r{}.scale+tlrc.HEAD'.format(sub, run) for run in runs]
deconvolve.inputs.out_file = 'stats'.format(sub)
deconvolve.inputs.x1D_uncensored = 'X.nocensor.xmat.1D'
deconvolve.inputs.num_stimts = 2
deconvolve.inputs.polort = 3
deconvolve.inputs.jobs = 2
deconvolve.inputs.fout =True 
deconvolve.inputs.tout = True 
deconvolve.inputs.x1D = 'X.xmat.1D'
deconvolve.inputs.xjpeg = 'X.jpg'
deconvolve.inputs.errts = 'errts.{}'.format(sub)
deconvolve.inputs.ortvec = [('mot_demean.r{}.1D'.format(run), 'mot_demean_r{}'.format(run)) for run in runs]
deconvolve.inputs.censor = 'censor_{}_combined_2.1D'.format(sub)
deconvolve.inputs.stim_times = [(1, "stimuli/AV1_vis.txt 'BLOCK(20,1)'"), (2, "stimuli/AV2_aud.txt 'BLOCK(20,1)'")]
deconvolve.inputs.stim_label = [(1, "vis"), (2, 'aud')]
deconvolve.inputs.gltsym = [('SYM: vis -aud'), ('SYM: 0.5*vis +0.5*aud')]
deconvolve.inputs.glt_label = [(1, 'V-A'), (2, 'mean.VA')]
deconvolve.cmdline
# should be
# 3dDeconvolve -input pb04.$subj.r*.scale+tlrc.HEAD                        \
#     -censor censor_${subj}_combined_2.1D                                 \
#     -ortvec mot_demean.r01.1D mot_demean_r01                             \
#     -ortvec mot_demean.r02.1D mot_demean_r02                             \
#     -ortvec mot_demean.r03.1D mot_demean_r03                             \
#     -polort 3                                                            \
#     -num_stimts 2                                                        \
#     -stim_times 1 stimuli/AV1_vis.txt 'BLOCK(20,1)'                      \
#     -stim_label 1 vis                                                    \
#     -stim_times 2 stimuli/AV2_aud.txt 'BLOCK(20,1)'                      \
#     -stim_label 2 aud                                                    \
#     -jobs 2                                                              \
#     -gltsym 'SYM: vis -aud'                                              \
#     -glt_label 1 V-A                                                     \
#     -gltsym 'SYM: 0.5*vis +0.5*aud'                                      \
#     -glt_label 2 mean.VA                                                 \
#     -fout -tout -x1D X.xmat.1D -xjpeg X.jpg                              \
#     -x1D_uncensored X.nocensor.xmat.1D                                   \
#     -errts errts.${subj}                                                 \
#     -bucket stats.$subj
res = deconvolve.run()  # doctest: +SKIP

"""missing
# if 3dDeconvolve fails, terminate the script
if ( $status != 0 ) then
    echo '---------------------------------------'
    echo '** 3dDeconvolve error, failing...'
    echo '   (consider the file 3dDeconvolve.err)'
    exit
endif
"""



# display any large pairwise correlations from the X-matrix
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
odt.inputs.in_file = 'X.xmat.1D'
odt.inputs.show_cormat_warnings = True
odt.inputs.out_file = "out.cormat_warn.txt"
odt.cmdline
# should be:
# 1d_tool.py -show_cormat_warnings -infile X.xmat.1D |& tee out.cormat_warn.txt
res = odt.run() 



# display degrees of freedom info from X-matrix
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
odt.inputs.in_file = 'X.xmat.1D'
odt.inputs.show_df_info = True
odt.inputs.out_file = "out.df_info.txt"
odt.cmdline
# should be:
# 1d_tool.py -show_df_info -infile X.xmat.1D |& tee out.df_info.txt
res = odt.run() 

# create an all_runs dataset to match the fitts, errts, etc.
tcat = afni.TCat() # Concatenate sub-bricks from input datasets into one big 3D+time dataset.
tcat.inputs.in_files = ['pb04.{}.r{}.scale+tlrc.HEAD'.format(sub, run) for run in runs]
tcat.inputs.out_file = "all_runs.{}".format(sub)
tcat.cmdline
# should be
# 3dTcat -prefix all_runs.$subj pb04.$subj.r*.scale+tlrc.HEAD
res = tcat.run()  


# --------------------------------------------------
# create a temporal signal to noise ratio dataset 
#    signal: if 'scale' block, mean should be 100
#    noise : compute standard deviation of errts
tcsb = afni.TCatSubBrick() # allow sub-brick selection 
tcsb.inputs.in_files = [('all_runs.{}+tlrc'.format(sub), "'[$ktrs]'")]
tcsb.inputs.out_file = "rm.signal.all"
tcsb.inputs.mean = True
# should be
# 3dTstat -mean -prefix rm.signal.all all_runs.$subj+tlrc"[$ktrs]"
res = tcsb.run() 

tcsb = afni.TCatSubBrick() # allow sub-brick selection 
tcsb.inputs.in_files = [('errts.{}+tlrc'.format(sub), "'[$ktrs]'")]
tcsb.inputs.out_file = "rm.noise.all"
tcsb.inputs.stdev = True
# should be
# 3dTstat -stdev -prefix rm.noise.all errts.${subj}+tlrc"[$ktrs]"
res = tcsb.run() 


calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
calc.inputs.in_file_a = 'rm.signal.all+tlrc'
calc.inputs.in_file_b = 'rm.noise.all+tlrc'
calc.inputs.expr = 'a/b'
calc.inputs.out_file = 'TSNR.{}'.format(sub)
calc.cmdline
# should be:
# 3dcalc -a rm.signal.all+tlrc                                             \
#        -b rm.noise.all+tlrc                                              \
#        -expr 'a/b' -prefix TSNR.$subj
res = calc.run()

# ---------------------------------------------------
# compute and store GCOR (global correlation average)
# (sum of squares of global mean of unit errts)
tnorm = afni.TNorm() # Shifts voxel time series from input so that seperate slices are aligned to the same temporal origin.
tnorm.inputs.in_file = 'functional.nii'
tnorm.inputs.norm2 = True
tnorm.inputs.out_file = 'rm.errts.unit errts.{}+tlrc'.format(sub)
tnorm.cmdline
# should be
# 3dTnorm -norm2 -prefix rm.errts.unit errts.${subj}+tlrc
res = tnorm.run()  

maskave = afni.Maskave() # Computes average of all voxels in the input dataset which satisfy the criterion in the options list
maskave.inputs.in_file = 'rm.errts.unit+tlrc'
maskave.inputs.mask= 'full_mask.{}+tlrc'.format(sub)
maskave.inputs.quiet= True
maskave.inputs.out_file = "mean.errts.unit.1D"
maskave.cmdline 
# should be 
# 3dmaskave -quiet -mask full_mask.$subj+tlrc rm.errts.unit+tlrc           \
#          > mean.errts.unit.1D
res = maskave.run()  

tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_files = 'mean.errts.unit.1D'
tstat.inputs.out_file = "out.gcor.1D"
tstat.sos = True
tstat.cmdline
# should be
# 3dTstat -sos -prefix - mean.errts.unit.1D\' > out.gcor.1D'
res = tstat.run()
print("-- GCOR = `cat out.gcor.1D`")


# ---------------------------------------------------
# compute correlation volume
# (per voxel: correlation with masked brain average)
maskave = afni.Maskave() # Computes average of all voxels in the input dataset which satisfy the criterion in the options list
maskave.inputs.in_file = ' errts.{}+tlrc'.format(sub)
maskave.inputs.mask = 'full_mask.{}+tlrc'.format(sub)
maskave.inputs.quiet = True
maskave.inputs.out_file = "mean.errts.1D"
maskave.cmdline 
# should be 
# 3dmaskave -quiet -mask full_mask.$subj+tlrc errts.${subj}+tlrc           \
#           > mean.errts.1D
res = maskave.run()  


tcorr1D = afni.TCorr1D() # Computes the correlation coefficient between each voxel time series in the input 3D+time dataset.
tcorr1D.inputs.xset= 'errts.{}+tlrc'.format(sub)
tcorr1D.inputs.y_1d = 'mean.errts.1D'
tcorr1D.inputs.out_file = 'corr_brain'
tcorr1D.cmdline
# should be
# 3dTcorr1D -prefix corr_brain errts.${subj}+tlrc mean.errts.1D
res = tcorr1D.run() 



# create fitts dataset from all_runs and errts
calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
calc.inputs.in_file_a = 'all_runs.{}+tlrc'.format(sub)
calc.inputs.in_file_b = 'errts.{}+tlrc'.format(sub)
calc.inputs.expr = 'a-b'
calc.inputs.out_file = 'fitts.{}'.format(sub)
calc.cmdline
# should be:
# 3dcalc -a all_runs.$subj+tlrc -b errts.${subj}+tlrc -expr a-b            \
#        -prefix fitts.$subj
res = calc.run()


# create ideal files for fixed response stim types
cat1d = afni.Cat()
# 1dcat takes as input one or more 1D files, and writes out a 1D file 
# containing the side-by-side concatenation of all or a subset of the columns from the input files.
cat1d.inputs.in_files = ["X.nocensor.xmat.1D'[12]'"]
cat1d.inputs.out_file = 'ideal_vis.1D'
cat1d.cmdline
res = cat1d.run()  

cat1d = afni.Cat()
# 1dcat takes as input one or more 1D files, and writes out a 1D file 
# containing the side-by-side concatenation of all or a subset of the columns from the input files.
cat1d.inputs.in_files = ["X.nocensor.xmat.1D'[13]'"]
cat1d.inputs.out_file = 'ideal_aud.1D'
cat1d.cmdline
res = cat1d.run()  


# --------------------------------------------------
# extract non-baseline regressors from the X-matrix,
# then compute their sum
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
odt.inputs.in_file = 'X.nocensor.xmat.1D'
odt.inputs.write_xstim = 'X.stim.xmat.1D'
odt.cmdline
# should be:
# 1d_tool.py -infile X.nocensor.xmat.1D -write_xstim X.stim.xmat.1D
res = odt.run() 


tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_files = 'X.stim.xmat.1D'
tstat.inputs.out_file = "sum_ideal.1D"
tstat.sum = True
tstat.cmdline
# should be
# 3dTstat -sum -prefix sum_ideal.1D X.stim.xmat.1D
res = tstat.run()

# ============================ blur estimation =============================
# compute blur estimates
from pathlib import Path
Path('blur_est.{}.1D'.format(sub)).touch() # start with empty file

# create directory for ACF curve files
os.mkdir('files_ACF')

# -- estimate blur for each run in epits --
Path('blur.epits.1D').touch()


# restrict to uncensored TRs, per run
for run in runs:
    odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
    odt.inputs.in_file = 'X.xmat.1D'
    odt.show_trs_uncensored = 'encoded'
    odt.show_trs_run = run
    odt.inputs.write_xstim = 'X.stim.xmat.1D'
    odt.cmdline
    # should be:
    # set trs = `1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded \
    #                       -show_trs_run $run`
    trs = odt.run()

    if trs == "":
        fwhm = afni.FWHMx() # Unlike the older 3dFWHM, this program computes FWHMs for all sub-bricks in the input dataset, each one separately.
        fwhm.inputs.in_file = 'all_runs.{}+tlrc"[trs]"'.format(sub)
        fwhm.inputs.detrend = True
        fwhm.inputs.mask = "mask_epi_anat.{}+tlrc".format(sub)
        fwhm.inputs.ACF = 'files_ACF/out.3dFWHMx.ACF.epits.r{}.1D'.format(run)
        fwhm.inputs.out_file = 'blur.epits.1D'
        fwhm.cmdline
        # should be
        # 3dFWHMx -detrend -mask mask_epi_anat.$subj+tlrc                      \
        # -ACF files_ACF/out.3dFWHMx.ACF.epits.r$run.1D                \
        # all_runs.$subj+tlrc"[$trs]" >> blur.epits.1D
        res = fwhm.run()  # doctest: +SKIP



# compute average FWHM blur (from every other row) and append
tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_files = "blur.epits.1D'{0..$(2)}'\'`"
tstat.mean = True
tstat.cmdline
# should be
# set blurs = ( `3dTstat -mean -prefix - blur.epits.1D'{0..$(2)}'\'` )
blurs = tstat.run()

print("average epits FWHM blurs: {}".format(blurs))
with open("blur_est.{}.1D".format(sub), "a") as f:
  print("{}    # epits FWHM blur estimates".format(blurs), file=f)



# -- estimate blur for each run in errts --
Path('blur.errts.1D').touch()


# restrict to uncensored TRs, per run
for run in runs:
    odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
    odt.inputs.in_file = 'X.xmat.1D'
    odt.show_trs_uncensored = 'encoded'
    odt.show_trs_run = run
    odt.inputs.write_xstim = 'X.stim.xmat.1D'
    odt.cmdline
    # should be:
    # set trs = `1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded \
    #                   -show_trs_run $run`
    trs = odt.run()

    if trs == "":
        fwhm = afni.FWHMx() # Unlike the older 3dFWHM, this program computes FWHMs for all sub-bricks in the input dataset, each one separately.
        fwhm.inputs.in_file = 'errts.{}+tlrc"[trs]"'.format(sub)
        fwhm.inputs.detrend = True
        fwhm.inputs.mask = "mask_epi_anat.{}+tlrc".format(sub)
        fwhm.inputs.ACF = 'files_ACF/out.3dFWHMx.ACF.errts.r{}.1D'.format(run)
        fwhm.inputs.out_file = 'blur.errts.1D'
        fwhm.cmdline
        # should be
        # 3dFWHMx -detrend -mask mask_epi_anat.$subj+tlrc                      \
        #         -ACF files_ACF/out.3dFWHMx.ACF.errts.r$run.1D                \
        #         errts.${subj}+tlrc"[$trs]" >> blur.errts.1D
        res = fwhm.run()  # doctest: +SKIP

# compute average FWHM blur (from every other row) and append
tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_files = "blur.errts.1D'{0..$(2)}'\'`"
tstat.mean = True
tstat.cmdline
# should be
# set blurs = ( `3dTstat -mean -prefix - blur.errts.1D'{0..$(2)}'\'` )
blurs = tstat.run()

print("average errts FWHM blurs: {}".format(blurs))
with open("blur_est.{}.1D".format(sub), "a") as f:
  print("{}    # errts FWHM blur estimates".format(blurs), file=f)

# compute average ACF blur (from every other row) and append
tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_files = "blur.errts.1D'{1..$(2)}'\'`"
tstat.mean = True
tstat.cmdline
# should be
# set blurs = ( `3dTstat -mean -prefix - blur.errts.1D'{1..$(2)}'\'` )
blurs = tstat.run()

print("average errts FWHM blurs: {}".format(blurs))
with open("blur_est.{}.1D".format(sub), "a") as f:
  print("{}    # errts ACF blur estimates".format(blurs), file=f)


# ========================= auto block: QC_review ==========================
# generate quality control review scripts and HTML report

"""missing
# generate a review script for the unprocessed EPI data
gen_epi_review.py -script @epi_review.$subj \
    -dsets pb00.$subj.r*.tcat+orig.HEAD
"""


# -------------------------------------------------
# generate scripts to review single subject results
# (try with defaults, but do not allow bad exit status)

"""missing
# write AP uvars into a simple txt file
cat << EOF > out.ap_uvars.txt
  mot_limit       : 0.3
  out_limit       : 0.05
  copy_anat       : FT_anat+orig.HEAD
  mask_dset       : mask_epi_anat.$subj+tlrc.HEAD
  tlrc_base       : TT_N27+tlrc.HEAD
  ss_review_dset  : out.ss_review.$subj.txt
  vlines_tcat_dir : vlines.pb00.tcat
EOF
"""



"""missing
# and convert the txt format to JSON
cat out.ap_uvars.txt | afni_python_wrapper.py -eval "data_file_to_json()" \
  > out.ap_uvars.json

# initialize gen_ss_review_scripts.py with out.ap_uvars.json
gen_ss_review_scripts.py -exit0        \
    -init_uvars_json out.ap_uvars.json \
    -write_uvars_json out.ss_review_uvars.json
"""


# ========================== auto block: finalize ==========================

"""missing
# remove temporary files
\rm -f rm.*
"""

"""missing
# if the basic subject review script is here, run it
# (want this to be the last text output)
if ( -e @ss_review_basic ) then
    ./@ss_review_basic |& tee out.ss_review.$subj.txt

    # generate html ss review pages
    # (akin to static images from running @ss_review_driver)
    apqc_make_tcsh.py -review_style pythonic -subj_dir . \
        -uvar_json out.ss_review_uvars.json
    tcsh @ss_review_html |& tee out.review_html
    apqc_make_html.py -qc_dir QC_$subj

    echo "\nconsider running: \n"
    echo "   afni_open -b $subj.results/QC_$subj/index.html"
    echo ""
endif

# return to parent directory (just in case...)
cd ..

echo "execution finished: `date`"

"""


# ==========================================================================
# script generated by the command:
#
# afni_proc.py -subj_id FT -blocks tshift align tlrc volreg blur mask scale   \
#     regress -radial_correlate_blocks tcat volreg -copy_anat FT/FT_anat+orig \
#     -dsets FT/FT_epi_r1+orig.HEAD FT/FT_epi_r2+orig.HEAD                    \
#     FT/FT_epi_r3+orig.HEAD -tcat_remove_first_trs 2 -align_unifize_epi      \
#     local -align_opts_aea -cost lpc+ZZ -giant_move -check_flip              \
#     -volreg_align_to MIN_OUTLIER -volreg_align_e2a -volreg_tlrc_warp        \
#     -volreg_compute_tsnr yes -blur_size 4.0 -mask_epi_anat yes              \
#     -regress_stim_times FT/AV1_vis.txt FT/AV2_aud.txt -regress_stim_labels  \
#     vis aud -regress_basis 'BLOCK(20,1)' -regress_censor_motion 0.3         \
#     -regress_censor_outliers 0.05 -regress_motion_per_run -regress_opts_3dD \
#     -jobs 2 -gltsym 'SYM: vis -aud' -glt_label 1 V-A -gltsym 'SYM: 0.5*vis  \
#     +0.5*aud' -glt_label 2 mean.VA -regress_compute_fitts                   \
#     -regress_make_ideal_sum sum_ideal.1D -regress_est_blur_epits            \
#     -regress_est_blur_errts -regress_run_clustsim no -html_review_style     \
#     pythonic -execute

















