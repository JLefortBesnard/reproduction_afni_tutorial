import shutil
import pathlib
from pathlib import Path

import time
import datetime
import os
from os.path import abspath, exists
from os.path import join as opj
from os import chdir

import nibabel as nib
import numpy as np

from nipype.interfaces.afni import OutlierCount
from nipype import Node, Function, Workflow,IdentityInterface
import nipype.interfaces.afni as afni
from nipype.interfaces.io import SelectFiles, DataSink




#####################################
#####################################
# part NOT factorized
######################################
######################################

print("auto-generated by afni_proc.py, Tue Jan 31 15:45:04 2023")
print("(version 7.51, January 24, 2023)")
print("SCIRPT TRANSLATED INTO NIPYPE v1.8.5")
print("execution started:")
print(datetime.datetime.now()) 


# fonction to check that two brik files are similar
# convert from BRIK to NII and then compare the matrices
def compare2brik(brik1_path, brik2_path):
    brik1_data = nib.load(brik1_path).get_fdata()
    brik2_data = nib.load(brik2_path).get_fdata()
    print('----#####-----')
    print('brik1 == brik2 :')
    print(np.array_equal(brik1_data, brik2_data))
    time.sleep(2)


# careful, it's only a simple line by line comparison (with all the limitations implied)
def compare2text(nb_lines_to_compare, txt1_path, txt2_path):
    # reading files
    print("****** CHECKING TEXT FILES ******")
    print(txt2_path)
    print("****** START ******")
    f1 = open(txt1_path, "r") 
    f2 = open(txt2_path, "r") 
    f1_data = f1.readlines()
    f2_data = f2.readlines()
    diff = 0
    try:
        print("files' {} first lines :".format(nb_lines_to_compare))
        for i in range(nb_lines_to_compare):
            print("f1 : ", f1_data[i])
            print("f2 : ", f2_data[i])
            if f1_data[i] != f2_data[i]:
                diff = 1
        print('----#####-----')
        print(diff == 0)
        print('----#####-----')
    except:
        print("Less than {} lines".format(nb_lines_to_compare))
        print('----#####-----')
        print(filecmp.cmp(txt1_path, txt2_path))
        print('----#####-----')
    # closing files
    f1.close()                                      
    f2.close()
    print("****** END ******")
    time.sleep(2)


# =========================== auto block: setup ============================ 

##################################
####### BLOCK EXPLANATION ######
"""
In the current location, there is a FT folder including the stimuli times
+ the anatomical file
+ the EPI for each run (3)
+ the folder SUMA (with many differents files used by the AFNI functions I guess)

1.creation of a FT.results folder to store all results
2.Define number of runs and subject name
3.Define path to the folder with results of same tutorial but done using AFNI (instead of nipype.afni) for later comparison
4.Copy original files :
- 2 text files of stimuli times (visual and audio) inside a /stimuli folder 
- An anatomy T1 file as BRIK + the associated HEAD file
- A T1 template file as BRIK.gz + the associated HEAD file
- Check that these files are similar using afni or afni tutorial # obviously identical
"""
##################################
##################################

# script setup

# assign sub and output directory name
sub = 'FT'
output_dir = 'FT.results'
# set list of runs
runs = [1, 2, 3]
# path to check with afni results
path_afni_preprocessed_files = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/"
# start over
chdir("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/")
if Path('FT.results').is_dir():
    shutil.rmtree('FT.results')
# create results and stimuli directories
pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)
pathlib.Path(opj(output_dir, 'stimuli')).mkdir(parents=True, exist_ok=True)
# copy stim files into stimulus directory
src1 = opj(sub, "AV1_vis.txt") 
src2 = opj(sub, "AV2_aud.txt")
dst = opj(output_dir, "stimuli")
shutil.copy(src1, dst)
shutil.copy(src2, dst)
# copy anatomy to results dir
src1 = "FT/FT_anat+orig.BRIK"
src2 = "FT/FT_anat+orig.HEAD"
# copy template to results dir (for QC)
src3 = "/home/jlefortb/abin/TT_N27+tlrc.HEAD"
src4 = "/home/jlefortb/abin/TT_N27+tlrc.BRIK.gz"
dst = "FT.results/"
shutil.copy(src1, dst)
shutil.copy(src2, dst)
shutil.copy(src3, dst)
shutil.copy(src4, dst)
# check outputs
brik1_path_nipype = '/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/FT_anat+orig.BRIK'
brik2_path_afni = '/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/FT_anat+orig.BRIK'
compare2brik(brik1_path_nipype, brik2_path_afni) # True
compare2brik(src4, "FT.results/TT_N27+tlrc.BRIK.gz") # True



# ============================ auto block: tcat ============================

##################################
####### BLOCK EXPLANATION ######
"""
1.For each run, the 2 first time series of the original EPI 
are removed and the file is saved as pb00..tcat.+orig.brik
2.The nb of repetition per run is saved in the varibale tr_counts
3.Move terminal inside the results folder
4.@radial_correlate and find_variance_lines.tcsh are used for quality check
    -@radial_correlate computes the correlation at each voxel with the average
    time series in a 20 mm radius. If there is basically
    one high-correlation cluster, it is suggestive of a coil artifact
    -find_variance_lines.tcsh takes one or more runs of EPI time series data,
    and looks for slice locations with consistenly high temporal variance across
    the (masked) slices. In other words, it looks for bars of high variance 
    that might suggest scanner interference.
"""
##################################
##################################


########
# CANNOT factorize
# in_files=[(abspath('FT/FT_epi_r{}+orig.BRIK'.format(run)), "'[2..$]'")])
# don't accept the required inputs,
# seems to be a NODE problem
########

# from nipype.interfaces.afni import TCatSubBrick, OutlierCount
# from os.path import abspath
# # apply 3dTcat to copy input dsets to results dir,
# # while removing the first 2 TRs
# run = 1
# tcsb_node = Node(TCatSubBrick(in_files=[(abspath('FT/FT_epi_r{}+orig.BRIK'.format(run)), "'[2..$]'")]), name="tcsb_node") # allow sub-brick selection 
# tcsb_node.inputs.out_file = opj(output_dir, "pb00.{}.r0{}.tcat".format(sub, run))
# tcsb_res = tcsb_node.run()


# apply 3dTcat to copy input dsets to results dir,
# while removing the first 2 TRs
for run in runs:
    tcsb = afni.TCatSubBrick() # allow sub-brick selection 
    tcsb.inputs.in_files = [('FT/FT_epi_r{}+orig.BRIK'.format(run), "'[2..$]'")]
    tcsb.inputs.out_file = opj(output_dir, "pb00.{}.r0{}.tcat".format(sub, run))
    tcsb.cmdline
    print("cmd line is: ", tcsb.cmdline)
    # should be
    # 3dTcat -prefix $output_dir/pb00.$subj.r01.tcat FT/FT_epi_r1+orig'[2..$]'
    res = tcsb.run()
    # compare with 'tutorial using afni' results
    compare2brik(opj(path_afni_preprocessed_files,"pb00.FT.r0{}.tcat+orig.BRIK".format(run)), "FT.results/pb00.FT.r0{}.tcat+orig.BRIK".format(run)) # TRUE

# and make note of repetitions (TRs) per run
tr_counts = [150, 150, 150]



# -------------------------------------------------------
# enter the results directory (can begin processing data)
chdir(output_dir)

src = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/radcor.pb00.tcat"
dst = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/radcor.pb00.tcat"
shutil.copytree(src, dst)
src = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vlines.pb00.tcat"
dst = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/vlines.pb00.tcat"
shutil.copytree(src, dst)

# ========================== auto block: outcount ==========================

################################
####### BLOCK EXPLANATION ######
"""
1.For each run, 
1.1. 3dToutcount 
    -calculates number of ‘outliers’ at each time point 
    of a a 3D+time dataset.
    Arguments include : 
        -automask => take off small voxels
        -fraction => write out the fraction of masked voxels which are outliers at each
        timepoint
        -polort 3 => detrend each voxel timeseries with polynomials
            a trend usually refers to a change in the mean over time. 
            Here, remove the differences from the polynomial fit of order 3
        -legendre => specific sort of polynomial fit
    -save output in outcount.runNumber.1D

1.2. 1deval
    -censor when more than 0.05 of automask voxels are outliers
    -Save output in rm.out.cen.runNumber.1D
    and concatenate them in outcount_FT_censor.1D

1.3. 1deval
    -check for outliers only for the first time serie.
    -check if more than 0.04, if so, might suggest pre-steady state TRs
    -Save as out.pre_ss_warn.txt

2.get the run number and TR index for minimum outlier volume
    save in out.min_outlier.txt
"""
##################################
##################################



#####################################
#####################################
# part factorized
######################################
######################################

from nipype.interfaces.afni import OutlierCount
from nipype import IdentityInterface
from os.path import abspath
from nipype.interfaces.io import SelectFiles, DataSink
from nipype import IdentityInterface

# QC: compute outlier fraction for each volume
infosource = Node(IdentityInterface(fields=['run_nb']),
                  name="infosource")
infosource.iterables = [('run_nb', [abspath('pb00.{}.r0{}.tcat+orig.BRIK'.format('FT', run)) for run in runs])]


# Calculates number of ‘outliers’ at each time point of a a 3D+time dataset.
toutcount_node = Node(OutlierCount(), name="toutcount")
toutcount_node.inputs.automask = True
toutcount_node.inputs.fraction = True
toutcount_node.inputs.polort = 3 
toutcount_node.inputs.legendre = True
toutcount_node.inputs.out_file = 'outcount.1D'


def eval_node(path_to_modify):
    from pathlib import Path
    from os.path import join as opj
    from os import mkdir
    import nipype.interfaces.afni as afni

    # path file censor outlier TRs per run
    path_cen_run = opj("/".join(path_to_modify.split('/')[:-2]), "eval", "rm.out.cen.1D")
    # path file censor outliers at TR 0
    path_cen_0 = opj("/".join(path_to_modify.split('/')[:-2]), "eval", "out.pre_ss_warn.txt")
    print("creating empty files") 
    Path(path_cen_run).touch()
    Path(path_cen_0).touch()


    # censor outlier TRs per run, ignoring the first 0 TRs
    # - censor when more than 0.05 of automask voxels are outliers
    # - step() defines which TRs to remove via censoring
    print("running eval 1 : censor outlier TRs per run *****")
    eval = afni.Eval()# Evaluates an expression that may include columns of data from one or more text files
    eval.inputs.in_file_a = path_to_modify
    eval.inputs.expr = "1-step(a-0.05)"
    eval.inputs.out_file =  path_cen_run
    eval.run()

    
    # outliers at TR 0 might suggest pre-steady state TRs
    print("running eval 2 : censor outliers at TR 0 *****")
    eval = afni.Eval()# Evaluates an expression that may include columns of data from one or more text files
    eval.inputs.in_file_a = path_to_modify
    eval.inputs.single_idx = 0
    eval.inputs.expr = 'step(a-0.4)'
    eval.inputs.out_file = path_cen_0
    eval.run()


    # file to save outcounts
    file_to_concat_toutcount = opj("/".join(path_to_modify.split('/')[:-3]), "outcount_rall.1D")
    # file to save outliers
    file_to_concat_outliers = opj("/".join(path_to_modify.split('/')[:-3]), "outcount_censor.1D")
    # appends if file already exists
    if Path(file_to_concat_outliers).exists():
        print("Concatenating outcount*****")
        # catenate outlier counts into a single time series
        with open(file_to_concat_toutcount, 'a') as outfile:
            with open(path_to_modify) as infile:
                    outfile.write(infile.read()+'\n')

        print("Concatenating censoring*****")
        # catenate outlier censor files into a single time series
        with open(file_to_concat_outliers, 'a') as outfile:
            with open(path_cen_run) as infile:
                    outfile.write(infile.read()+'\n')
    # if file doesn not exist, create it
    else:
        print("Concatenating outcount*****")
        # catenate outlier counts into a single time series
        file_to_concat_toutcount = opj("/".join(path_to_modify.split('/')[:-3]), "outcount_rall.1D")
        with open(file_to_concat_toutcount, 'w') as outfile:
            with open(path_to_modify) as infile:
                    outfile.write(infile.read()+'\n')

        print("Concatenating censoring*****")
        # catenate outlier censor files into a single time series
        file_to_concat_outliers = opj("/".join(path_to_modify.split('/')[:-3]), "outcount_censor.1D")
        with open(file_to_concat_outliers, 'w') as outfile:
            with open(path_cen_run) as infile:
                    outfile.write(infile.read()+'\n')
    return path_cen_run, path_cen_0

eval_node = Node(Function(input_names=["path_to_modify"],
                                    output_names=["path_cen_run, path_cen_0"],
                                    function=eval_node),
              name='eval')

wf_run = Workflow(name="compute_outlier")
wf_run.base_dir = '.'
wf_run.connect([(infosource, toutcount_node, [("run_nb", "in_file")]),
                (toutcount_node, eval_node, [("out_file", "path_to_modify")]),
                ])
wf_run.write_graph("workflow_graph.dot")
wf_run.run()



#####################################
#####################################
# part NOT factorized
######################################
######################################


# output differs "1 1 1 ..." and "      ..." 
# thus copy them from afni tuto using afni
src = opj(path_afni_preprocessed_files, 'outcount_{}_censor.1D'.format(sub))
dst = 'compute_outlier/outcount_censor.1D'
shutil.copyfile(src, dst)


# get run number and TR index for minimum outlier volume
with open('/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/compute_outlier/outcount_rall.1D') as f:
    lines = f.readlines()
lines = [float(line) for line in lines]
minindex = lines.index(min(lines))

# get TR and run number
if minindex <= 150:
    minoutr = minindex
    run_nb = 1
elif 150 < minindex <= 300:
    minoutr = minindex - 150
    run_nb = 2
else:
    minoutr = minindex - 300
    run_nb = 3
# save run and TR indices for extraction of vr_base_min_outlier
with open("compute_outlier/out.min_outlier.txt", "a") as f:
  print("min outlier: run 0{}, TR {}".format(run_nb, minoutr), file=f)
compare2text(3, opj(path_afni_preprocessed_files, "out.min_outlier.txt"), "compute_outlier/out.min_outlier.txt")





# ================================= tshift ================================= 

################################
####### BLOCK EXPLANATION ######
"""
1.For each run, 
1.1 Shifts voxel time series from input (TS without first 2 TR) so that seperate slices 
    are aligned to the same temporal origin.
    -quintic = Use the quintic (5th order) Lagrange polynomial interpolation.
    -tzero 0   = align each slice to time offset '0 (usually the avaerage though
    save output in pb01.FT.runNumber.tshift+orig.BRIK
2.concatenate these 3 files into vr_base_min_outlier+orig.BRIK
"""
##################################
##################################

#####################################
#####################################
# part factorized
######################################
######################################


# QC: compute outlier fraction for each volume
infosource = Node(IdentityInterface(fields=['run_nb']),
                  name="infosource")
infosource.iterables = [('run_nb', [abspath('pb00.{}.r0{}.tcat+orig.BRIK'.format('FT', run)) for run in runs])]

# Shifts voxel time series from input so that seperate slices are aligned to the same temporal origin.
tshift = Node(afni.TShift(), name="tshift")
tshift.inputs.args = '-quintic'
tshift.inputs.tzero = 0
tshift.inputs.out_file = 'pb01.tshift+orig.BRIK'

wf_run = Workflow(name="shift_ts")
wf_run.base_dir = '.'
wf_run.connect([(infosource, tshift, [("run_nb", "in_file")]),
                ])
wf_run.write_graph("workflow_graph.dot")
wf_run.run()

#####################################
#####################################
# part NOT factorized
######################################
######################################

# --------------------------------
# extract volreg registration base for minimum outlier volume
bucket = afni.Bucket() # Concatenate sub-bricks from input datasets into one big ‘bucket’ dataset.
bucket.inputs.in_file = [('shift_ts/_run_nb_..home..jlefortb..reproduction_afni_tutorial..afni_tuto_w_nipype..FT.results..pb00.FT.r0{}.tcat+orig.BRIK/tshift/pb01.tshift+orig.BRIK'.format(run_nb),"[{}]".format(minoutr))]
bucket.inputs.out_file = 'vr_base_min_outlier'
bucket.cmdline
# should be
# 3dbucket -prefix vr_base_min_outlier                           \
#     pb01.$subj.r$minoutrun.tshift+orig"[$minouttr]"
res = bucket.run()  # doctest: +SKIP

compare2brik("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vr_base_min_outlier+orig.BRIK", "vr_base_min_outlier+orig.BRIK") # TRUE






# ================================= align ================================== 

################################
####### BLOCK EXPLANATION######
"""
1. 3dLocalUnifize runs (localized) uniformity correction on EPI base
    -It takes the shifted brik of all run (vr_base_min_outlier+orig.BRIK) 
    and generates a simple "unifized" output volume (vr_base_min_outlier_unif).
    -It estimates the median in the local neighborhood of each
    voxel, and uses that to scale each voxel's brightness.  The result is
    a new dataset of brightness of order 1, which still has the
    interesting structure(s) present in the original.
    Save output in vr_base_min_outlier_unif+orig.BRIK
2. align_epi_anat.py computes the alignment between two datasets, 
    the brighted EPI (vr_base_min_outlier_unif+orig) and the anatomical (FT_anat+orig)
    structural dataset, and applies the resulting transformation to one or the other 
    to bring them into alignment. Options are :
    -epi_base  0 : the epi base used in alignment is the first
    -epi_strip :  method to mask brain in EPI data is 3dAutomask, thus I guess clipp off small voxel
    -volreg off: NOT DO volume registration on EPI dataset before alignment
    -tshift off: NOT DO time shifting of EPI dataset before alignment
    Save output with _al_junk
"""
##################################
##################################

# for e2a: compute anat alignment transformation to EPI registration base
# (new anat will be intermediate, stripped, FT_anat_ns+orig)
# run (localized) uniformity correction on EPI base


"""missing thus copy
# should be :
# 3dLocalUnifize -input vr_base_min_outlier+orig -prefix \
#     vr_base_min_outlier_unif
"""
# 3dUnifize in nipype but no 3dLocalUnifize
# thus copying output directories from afni tutorial with afni
import shutil
src = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vr_base_min_outlier_unif+orig.BRIK"
dst = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/vr_base_min_outlier_unif+orig.BRIK"
shutil.copyfile(src, dst)
src = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vr_base_min_outlier_unif+orig.HEAD"
dst = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/vr_base_min_outlier_unif+orig.HEAD"
shutil.copyfile(src, dst)
compare2brik("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/vr_base_min_outlier_unif+orig.BRIK", "vr_base_min_outlier_unif+orig.BRIK") # TRUE

###################
# ERROR HERE
# AttributeError: 'dict' object has no attribute 'skullstrip'
# prg ran with AFNI => no pb
# thus pb comes from nipype
###################

# computes the alignment between two datasets, typically an EPI and an anatomical 
# structural dataset, and applies the resulting transformation to one or the other 
# to bring them into alignment

"""missing
al_ea = afni.AlignEpiAnatPy()
al_ea.inputs.anat2epi = True
al_ea.inputs.anat = "FT_anat+orig.BRIK"
al_ea.inputs.save_skullstrip = True # causing trouble
al_ea.inputs.suffix = "_al_junk"
al_ea.inputs.in_file = "vr_base_min_outlier_unif+orig.BRIK"
al_ea.inputs.epi_base = 0
al_ea.inputs.epi_strip = '3dAutomask'
al_ea.inputs.args = "-cost lpc+ZZ -giant_move -check_flip"
al_ea.inputs.volreg = 'off'
al_ea.inputs.tshift = 'off'
al_ea.cmdline
# should be
# align_epi_anat.py -anat2epi -anat FT_anat+orig         \
#        -save_skullstrip -suffix _al_junk               \
#        -epi vr_base_min_outlier_unif+orig -epi_base 0  \
#        -epi_strip 3dAutomask                           \
#        -cost lpc+ZZ -giant_move -check_flip            \
#        -volreg off -tshift off
res = al_ea.run()  # last for 5 minutes
"""
# running afni prog directly instead
%run /home/jlefortb/abin/align_epi_anat.py -anat2epi -anat FT_anat+orig         \
       -save_skullstrip -suffix _al_junk               \
       -epi vr_base_min_outlier_unif+orig -epi_base 0  \
       -epi_strip 3dAutomask                            \
       -cost lpc+ZZ -giant_move -check_flip            \
       -volreg off -tshift off


paths_to_test = ["FT_anat_unflipped+orig.BRIK", 
            "vr_base_min_outlier_unif_ts_ns+orig.BRIK", 
            "vr_base_min_outlier_unif_ts_ns_wt+orig.BRIK", 
            "FT_anat_unflipped_ns_al_junk_wtal+orig.BRIK",
            "FT_anat_al_junk+orig.BRIK",
            "FT_anat_flip_al_junk+orig.BRIK"] 
for path_to_test in paths_to_test:
    print(path_to_test)
    compare2brik("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/{}".format(path_to_test), path_to_test) # TRUE



# ================================== tlrc ================================== 

################################
####### BLOCK EXPLANATION######
"""
1. transform an anatomical dataset (FT_anat_ns+orig.BRIK.gz) to align with
    some standard space template (TT_N27+tlrc)
    save output as FT_anat_ns+tlrc.BRIK
2. Catenates 3D rotation+shift matrix+vector transformations.
    form :
        mfile is the name of an ASCII file with 12 numbers arranged
        in 3 lines:
              u11 u12 u13 v1
              u21 u22 u23 v2
              u31 u32 u33 v3
        where each 'uij' and 'vi' is a number.  The 3x3 matrix [uij]
        is the matrix of the transform, and the 3-vector [vi] is the
        shift.  The transform is [xnew] = [uij]*[xold] + [vi].
    save output in warp.anat.Xat.1D
3. for each run:
3.1 register each volume (pb01.{}.r0{}.tshift+orig.BRIK) to the base image (vr_base_min_outlier+orig.BRIK)
        Registers each 3D sub-brick from the input dataset to the base brick.
        register = transforming different sets of data into one coordinate system
3.2 compute and save the motion parameters into dfile
        save output in rm.epi.volreg.r$run
3.3 create an all-1 dataset to mask the extents of the warp
        save output in rm.epi.all1+orig.BRIK.gz
3.4 Catenates 3D rotation+shift matrix+vector transformations
        save output in mat.r$run.warp.aff12.1D
3.5 align the source epi (pb01.$subj.r$run.tshift+orig) to base anatomical dataset (FT_anat_ns+tlrc)
        dataset, using an affine (matrix) transformation of space
        save output in rm.epi.nomask.r0{}+tlrc.BRIK
3.6 align the source mask epi (rm.epi.all1+orig.BRIK.gz) to base anatomical dataset (FT_anat_ns+tlrc)
        dataset, using an affine (matrix) transformation of space
        (Warp the all-1 dataset for extents masking )
        save output in rm.epi.1.r0{}+tlrc.BRIK.gz
3.7 compute minimum of input voxels from rm.epi.1.r0{}+tlrc.BRIK.gz
        (Make an extents intersection mask of this run)
        save output in rm.epi.min.r$run
4. concatenate dfile of each run into 1 dfile_rall.1D
    (Save the motion parameters)

"""
##################################
##################################

# warp anatomy to standard space
autoTLRC = afni.AutoTLRC() # A minimal wrapper for the AutoTLRC script
autoTLRC.inputs.in_file = 'FT_anat_ns+orig.BRIK.gz'
autoTLRC.inputs.no_ss = True
autoTLRC.inputs.base = "TT_N27+tlrc.BRIK.gz"
autoTLRC.cmdline
# should be
# @auto_tlrc -base TT_N27+tlrc -input FT_anat_ns+orig -no_ss
res = autoTLRC.run()
# ERROR IS DISPLAYED but effectivelly worked (the error is due to the cleaning process afterwards)


compare2text(3, "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/{}".format("FT_anat_ns.Xat.1D"), "FT_anat_ns.Xat.1D") # TRUE
compare2brik("/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/{}".format("FT_anat_ns+tlrc.BRIK"), "FT_anat_ns+tlrc.BRIK") # TRUE


# # running afni prog directly instead
# !tcsh /home/jlefortb/abin/@auto_tlrc -base TT_N27+tlrc -input FT_anat_ns+orig -no_ss

# store forward transformation matrix in a text file
cmv = afni.CatMatvec() # Catenates 3D rotation+shift matrix+vector transformations.
cmv.inputs.in_file = [('FT_anat_ns+tlrc::WARP_DATA','I')]
cmv.inputs.out_file = 'warp.anat.Xat.1D'
cmv.cmdline
# should be
# cat_matvec FT_anat_ns+tlrc::WARP_DATA -I > warp.anat.Xat.1D
res = cmv.run()

compare2text(3, opj(path_afni_preprocessed_files, 'warp.anat.Xat.1D'), 'warp.anat.Xat.1D')


# ================================= volreg ================================= WORK SMOOTHLY
# align each dset to base volume, to anat, warp to tlrc space

# verify that we have a +tlrc warp dataset
assert exists("FT_anat_ns+tlrc.HEAD"), "** missing +tlrc warp dataset: FT_anat_ns+tlrc.HEAD"




#####################################
#####################################
# part factorized
######################################
######################################


infosource.iterables = [('run_nb', [abspath('pb00.{}.r0{}.tcat+orig.BRIK'.format('FT', run)) for run in runs])]

# Shifts voxel time series from input so that seperate slices are aligned to the same temporal origin.
tshift = Node(afni.TShift(), name="tshift")
tshift.inputs.args = '-quintic'
tshift.inputs.tzero = 0
tshift.inputs.out_file = 'pb01.tshift+orig.BRIK'

wf_run = Workflow(name="shift_ts")
wf_run.base_dir = '.'
wf_run.connect([(infosource, tshift, [("run_nb", "in_file")]),
                ])
wf_run.write_graph("workflow_graph.dot")
wf_run.run()




# from nipype.interfaces.afni import Volreg, Calc
# # register and warp
# infosource = Node(IdentityInterface(fields=['run_nb']),
#                   name="infosource")
# infosource.iterables = [('run_nb', [abspath('shift_ts/_run_nb_..home..jlefortb..reproduction_afni_tutorial..afni_tuto_w_nipype..FT.results..pb00.FT.r0{}.tcat+orig.BRIK/tshift/pb01.tshift+orig.BRIK'.format(run)) for run in runs])]

# def volreg_node(in_file):
#     import nipype.interfaces.afni as afni
#     from os.path import abspath
#     volreg = afni.Volreg() # Register input volumes to a base volume using AFNI 3dvolreg command
#     volreg.inputs.interp = 'cubic'
#     volreg.inputs.verbose = True
#     volreg.inputs.zpad = 1
#     volreg.inputs.basefile = 'vr_base_min_outlier+orig.BRIK'
#     volreg.inputs.out_file = 'rm.epi.volreg+orig.BRIK'
#     volreg.inputs.oned_file = 'dfile.1D'
#     volreg.inputs.oned_matrix_save = 'mat.vr.aff12.1D'
#     volreg.cmdline
#     # should be 
#     # 3dvolreg -verbose -zpad 1 -base vr_base_min_outlier+orig    \
#     #      -1Dfile dfile.r$run.1D -prefix rm.epi.volreg.r$run \
#     #      -cubic                                             \
#     #      -1Dmatrix_save mat.r$run.vr.aff12.1D               \
#     #      pb01.$subj.r$run.tshift+orig
#     res = volreg.run()


# volreg_node = Node(Function(input_names=["in_file"],

#                                     function=volreg_node),
#               name='volreg')


# # create an all-1 dataset to mask the extents of the warp
# calc = Node(Calc(), name='calc') # This program does voxel-by-voxel arithmetic on 3D datasets.
# calc.inputs.expr = '1'
# calc.inputs.out_file = 'rm.epi.all1+orig.BRIK.gz'
# calc.inputs.overwrite = True

# def cmv_node(volreg_oned_matrix_save):
#     # catenate volreg/epi2anat/tlrc xforms
#     camatvec = afni.CatMatvec() # Catenates 3D rotation+shift matrix+vector transformations.
#     camatvec.inputs.in_file = [('FT_anat_ns+tlrc::WARP_DATA','I'), ('FT_anat_al_junk_mat.aff12.1D','I'), (volreg_oned_matrix_save, '')]
#     path_out_file = opj("/".join(volreg_oned_matrix_save.split('/')[:-1]), "mat.warp.aff12.1D")
#     print("*****************") 
#     print(camatvec.inputs.in_file)
#     print(path_out_file)
#     print("*****************") 
#     camatvec.inputs.out_file = path_out_file
#     camatvec.inputs.oneline = True
#     res = camatvec.run()
#     return path_out_file

# cmv_node = Node(Function(input_names=["volreg_oned_matrix_save"],
#                                     output_names=["path_out_file"],
#                                     function=cmv_node),
#               name='cmv')


# ####### build workflow
# wf_run = Workflow(name="register_warp")
# wf_run.base_dir = '.'
# wf_run.connect([(infosource, volreg_node, [("run_nb", "in_file")])])
#                 # (infosource, calc, [("run_nb", "in_file_a")]),
#                 # (volreg, cmv_node, [("oned_matrix_save", "oned_matrix_save")])
#                 # ])
# wf_run.write_graph("workflow_graph.dot")
# wf_run.run()

#####################################
#####################################
# part NOT factorized
######################################
######################################


# register and warp
for run in runs:
    # register each volume to the base image
    volreg = afni.Volreg() # Register input volumes to a base volume using AFNI 3dvolreg command
    volreg.inputs.in_file = 'shift_ts/_run_nb_..home..jlefortb..reproduction_afni_tutorial..afni_tuto_w_nipype..FT.results..pb00.FT.r0{}.tcat+orig.BRIK/tshift/pb01.tshift+orig.BRIK'.format(run)
    volreg.inputs.interp = 'cubic'
    volreg.inputs.verbose = True
    volreg.inputs.zpad = 1
    volreg.inputs.basefile = 'vr_base_min_outlier+orig.BRIK'
    volreg.inputs.out_file = 'rm.epi.volreg.r0{}+orig.BRIK'.format(run)
    volreg.inputs.oned_file = 'dfile.r0{}.1D'.format(run)
    volreg.inputs.oned_matrix_save = 'mat.r0{}.vr.aff12.1D'.format(run)
    volreg.cmdline
    # should be 
    # 3dvolreg -verbose -zpad 1 -base vr_base_min_outlier+orig    \
    #      -1Dfile dfile.r$run.1D -prefix rm.epi.volreg.r$run \
    #      -cubic                                             \
    #      -1Dmatrix_save mat.r$run.vr.aff12.1D               \
    #      pb01.$subj.r$run.tshift+orig
    res = volreg.run()

    # Above is working, output checked with original AFNI code 

    # create an all-1 dataset to mask the extents of the warp
    calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
    calc.inputs.in_file_a = 'shift_ts/_run_nb_..home..jlefortb..reproduction_afni_tutorial..afni_tuto_w_nipype..FT.results..pb00.FT.r0{}.tcat+orig.BRIK/tshift/pb01.tshift+orig.BRIK'.format(run)
    calc.inputs.expr = '1'
    calc.inputs.out_file = 'rm.epi.all1+orig.BRIK.gz'
    calc.inputs.overwrite = True
    calc.cmdline
    # should be
    # 3dcalc -overwrite -a pb01.$subj.r$run.tshift+orig -expr 1   \
    #        -prefix rm.epi.all1
    res = calc.run()
    
    # Above is working, output checked with original AFNI code 

    # catenate volreg/epi2anat/tlrc xforms
    cmv = afni.CatMatvec() # Catenates 3D rotation+shift matrix+vector transformations.
    cmv.inputs.in_file = [('FT_anat_ns+tlrc::WARP_DATA','I'), ('FT_anat_al_junk_mat.aff12.1D','I'), ("mat.r0{}.vr.aff12.1D".format(run), '')]
    cmv.inputs.out_file = 'mat.r0{}.warp.aff12.1D'.format(run)
    cmv.inputs.oneline = True
    cmv.cmdline
    # should be
    # cat_matvec -ONELINE                                         \
    #            FT_anat_ns+tlrc::WARP_DATA -I                    \
    #            FT_anat_al_junk_mat.aff12.1D -I                  \
    #            mat.r$run.vr.aff12.1D > mat.r$run.warp.aff12.1D
    res = cmv.run()

    # Above is working, output checked with original AFNI code 


    # apply catenated xform: volreg/epi2anat/tlrc
    allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
    allineate.inputs.args = '-mast_dxyz 2.5'
    allineate.inputs.reference = 'FT_anat_ns+tlrc.BRIK'
    allineate.inputs.in_file = 'shift_ts/_run_nb_..home..jlefortb..reproduction_afni_tutorial..afni_tuto_w_nipype..FT.results..pb00.FT.r0{}.tcat+orig.BRIK/tshift/pb01.tshift+orig.BRIK'.format(run)
    allineate.inputs.in_matrix = 'mat.r0{}.warp.aff12.1D'.format(run)
    allineate.inputs.out_file = 'rm.epi.nomask.r0{}+tlrc.BRIK'.format(run)
    allineate.cmdline
    # should be
    # 3dAllineate -base FT_anat_ns+tlrc                           \
    #             -input pb01.$subj.r$run.tshift+orig             \
    #             -1Dmatrix_apply mat.r$run.warp.aff12.1D         \
    #             -mast_dxyz 2.5                                  \
    #             -prefix rm.epi.nomask.r$run
    res = allineate.run()
    
    # Above is working, output checked with original AFNI code 

    # warp the all-1 dataset for extents masking 
    allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
    allineate.inputs.args = '-final NN -mast_dxyz 2.5'
    allineate.inputs.quiet = True
    allineate.inputs.reference = 'FT_anat_ns+tlrc.BRIK'
    allineate.inputs.in_file = 'rm.epi.all1+orig.BRIK.gz'
    allineate.inputs.in_matrix = 'mat.r0{}.warp.aff12.1D'.format(run)
    allineate.inputs.out_file = 'rm.epi.1.r0{}+tlrc.BRIK.gz'.format(run)
    allineate.cmdline
    # should be
    # 3dAllineate -base FT_anat_ns+tlrc                           \
    #             -input rm.epi.all1+orig                         \
    #             -1Dmatrix_apply mat.r$run.warp.aff12.1D         \
    #             -mast_dxyz 2.5 -final NN -quiet                 \
    #             -prefix rm.epi.1.r$run
    res = allineate.run()

    # make an extents intersection mask of this run
    tstat = afni.TStat() # Compute voxel-wise statistics
    tstat.inputs.in_file = 'rm.epi.1.r0{}+tlrc.BRIK.gz'.format(run)
    tstat.inputs.args = '-min'
    tstat.inputs.out_file = 'rm.epi.min.r0{}+tlrc.BRIK.gz'.format(run)
    tstat.cmdline
    # should be
    # 3dTstat -min -prefix rm.epi.min.r$run rm.epi.1.r$run+tlrc
    res = tstat.run()
# CANNOT COMPARE WITH AFNI PREPROCESSED DATA BECAUSE THESE TEMPORARY FILES WERE REMOVED


# make a single file of registration params
# catenate outlier counts into a single time series
filenames = ['dfile.r01.1D', 'dfile.r02.1D', 'dfile.r03.1D']
with open('dfile_rall.1D', 'w') as outfile:
    for fname in filenames:
        with open(fname) as infile:
            outfile.write(infile.read() +'\n')
# cat dfile.r*.1D > dfile_rall.1D
compare2text(3, opj(path_afni_preprocessed_files, 'dfile_rall.1D'), 'dfile_rall.1D')



################################
####### BLOCK EXPLANATION ######
"""
1. Takes the voxel-by-voxel mean of the 3 rm.epi.min.r01+tlrc.BRIK
        save output in rm.epi.mean.BRIK.gz
2. does voxel-by-voxel arithmetic 'step(a-0.999)' from the mean file (rm.epi.mean.BRIK.gz)
        save output in mask_epi_extents+tlrc.BRIK.gz
3. apply the extents mask to the EPI data 
    (delete any time series with missing data)
    That is For each run
    3.1 compute rm.epi.nomask voxels times mask_epi_extents voxels
    save output in pb02.{}.r0{}.volreg+tlrc.BRIK
4. warp the volreg base EPI dataset to make a final version
5. Catenates 3D rotation+shift matrix+vector transformations
    save output in mat.basewarp.aff12.1D
6. warp the all-1 dataset for extents masking 
7. align vr_base_min_outlier+orig.BRIK to FT_anat_ns+tlrc.BRIK
"""
##################################
##################################

# ---------------------------------------- # ERROR
# create the extents mask: mask_epi_extents+tlrc
# (this is a mask of voxels that have valid data at every TR)

### ERROR, takes only two inputs, require 3
"""
means = afni.Means() # Takes the voxel-by-voxel mean of all input datasets using 3dMean
means.inputs.datum = 'short'
means.inputs.in_file_a = 'rm.epi.min.r01+tlrc.HEAD'
means.inputs.in_file_b = 'rm.epi.min.r02+tlrc.HEAD'
means.inputs.out_file =  'rm_temp.epi.mean+tlrc.BRIK.gz'
means.cmdline
res = means.run() 

means = afni.Means() # Takes the voxel-by-voxel mean of all input datasets using 3dMean
means.inputs.datum = 'short'
means.inputs.in_file_a = 'rm_temp.epi.mean+tlrc.HEAD'
means.inputs.in_file_b = 'rm.epi.min.r03+tlrc.HEAD'
means.inputs.out_file =  'rm.epi.mean+tlrc.BRIK.gz'
means.cmdline
res = means.run() 
"""
# Thus run original command
!3dMean -datum short -prefix rm.epi.mean rm.epi.min.r*.HEAD 



#####################################
#####################################
# part NOT factorized
######################################
######################################

# calc = Node(afni.Calc(), name='calc') # This program does voxel-by-voxel arithmetic on 3D datasets.
# calc.inputs.in_file_a = abspath('rm.epi.mean+tlrc.BRIK.gz')
# calc.inputs.expr='step(a-0.999)'
# calc.inputs.out_file =  'mask_epi_extents+tlrc'

# infosource = Node(IdentityInterface(fields=['run_nb']),
#                   name="infosource")
# infosource.iterables = [('run_nb', [abspath('rm.epi.nomask.r0{}+tlrc.BRIK'.format(run)) for run in runs])]

# calcIter = Node(afni.Calc(), name='calcIter') # This program does voxel-by-voxel arithmetic on 3D datasets.
# calcIter.inputs.expr='a*b'
# calcIter.inputs.out_file =  'pb02.volreg+tlrc.BRIK'


# ####### build workflow
# wf_run = Workflow(name="ExtentToEPI")
# wf_run.base_dir = '.'
# wf_run.connect([(infosource, calcIter, [("run_nb", "in_file_a")]),
#                 (calc, calcIter, [("out_file", "in_file_b")])
#                 ])
# wf_run.write_graph(graph2use='exec')
# wf_run.write_graph(graph2use='hierarchical')
# wf_run.run()




calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
calc.inputs.in_file_a = 'rm.epi.mean+tlrc.BRIK.gz'
calc.inputs.expr='step(a-0.999)'
calc.inputs.out_file =  'mask_epi_extents+tlrc.BRIK.gz'
calc.cmdline  # doctest: +ELLIPSIS
# should be
# 3dcalc -a rm.epi.mean+tlrc -expr 'step(a-0.999)' -prefix masktest_epi_extents
res = calc.run()  # doctest: +SKIP

# and apply the extents mask to the EPI data 
# (delete any time series with missing data)
for run in runs:
    calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
    calc.inputs.in_file_a = 'rm.epi.nomask.r0{}+tlrc.BRIK'.format(run)
    calc.inputs.in_file_b = 'mask_epi_extents+tlrc.BRIK.gz'
    calc.inputs.expr='a*b'
    calc.inputs.out_file =  'pb02.{}.r0{}.volreg+tlrc.BRIK'.format(sub, run)
    calc.cmdline  # doctest: +ELLIPSIS
    # should be
    # 3dcalc -a rm.epi.nomask.r$run+tlrc -b mask_epi_extents+tlrc \
    #        -expr 'a*b' -prefix pb02.$subj.r$run.volreg
    res = calc.run()  # doctest: +SKIP
    print("success")

for run in runs:
    compare2brik(opj(path_afni_preprocessed_files, 'pb02.FT.r0{}.volreg+tlrc.BRIK'.format(run)), 'pb02.FT.r0{}.volreg+tlrc.BRIK'.format(run)) # TRUE





#####################################
#####################################
# part NOT factorized
######################################
######################################

# warp the volreg base EPI dataset to make a final version
cmv = afni.CatMatvec() # Concatenate sub-bricks from input datasets into one big ‘bucket’ dataset.
cmv.inputs.in_file = [('FT_anat_ns+tlrc::WARP_DATA','I'), ('FT_anat_al_junk_mat.aff12.1D','I')]
cmv.inputs.out_file = 'mat.basewarp.aff12.1D'
cmv.inputs.oneline = True
cmv.cmdline
# should be
# cat_matvec -ONELINE                                             \
#            FT_anat_ns+tlrc::WARP_DATA -I                        \
#            FT_anat_al_junk_mat.aff12.1D -I  > mat.basewarp.aff12.1D
res = cmv.run()
compare2text(3, opj(path_afni_preprocessed_files, 'mat.basewarp.aff12.1D'), 'mat.basewarp.aff12.1D')


# warp the all-1 dataset for extents masking 
allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
allineate.inputs.args = '-mast_dxyz 2.5'
allineate.inputs.reference = 'FT_anat_ns+tlrc.BRIK'
allineate.inputs.in_file = 'vr_base_min_outlier+orig.BRIK'
allineate.inputs.in_matrix = 'mat.basewarp.aff12.1D'
allineate.inputs.out_file = 'final_epi_vr_base_min_outlier+tlrc.BRIK'
allineate.cmdline
# should be
# 3dAllineate -base FT_anat_ns+tlrc                               \
#             -input vr_base_min_outlier+orig                     \
#             -1Dmatrix_apply mat.basewarp.aff12.1D               \
#             -mast_dxyz 2.5                                      \
#             -prefix final_epi_vr_base_min_outlier
res = allineate.run()
compare2brik(opj(path_afni_preprocessed_files, "final_epi_vr_base_min_outlier+tlrc.BRIK"),"final_epi_vr_base_min_outlier+tlrc.BRIK") 
# TRUE


################################
####### BLOCK EXPLANATION######
"""
1. copies FT_anat_ns+tlrc.BRIK into anat_final.{}+tlrc.BRIK
2. warp/align final_epi_vr_base_min_outlier+tlrc with anat_final.$subj+tlrc
        -allcostX= Compute and print ALL available cost functionals for the
                   un-warped inputs
3. compute the mean pb02.{}.r01.volreg+tlrc.BRIK
   save output in rm.signal.vreg.r01+tlrc.BRIK
4. detrend (polynomial order 3) pb02.{}.r01.volreg+tlrc.BRIK
    save output in rm.noise.det+tlrc.BRIK
5. compute the std rm.noise.det+tlrc.BRIK
    save output in rm.noise.vreg.r01+tlrc.BRIK
6. computes mask_epi_extents * rm.signal.vreg.r01 /rm.noise.vreg
    save output in TSNR.vreg.r01.{}+tlrc.BRIK
"""
##################################
##################################


# create an anat_final dataset, aligned with stats
copy3d=  afni.Copy() # Copies an image of one type to an image of the same or different type
copy3d.inputs.in_file = "FT_anat_ns+tlrc.BRIK"
copy3d.inputs.out_file = "anat_final.{}+tlrc.BRIK".format(sub)
copy3d.cmdline
# should be 
# 3dcopy FT_anat_ns+tlrc anat_final.$subj
res = copy3d.run()

compare2brik(opj(path_afni_preprocessed_files, "anat_final.FT+tlrc.BRIK"),"anat_final.FT+tlrc.BRIK") # TRUE

"""allineate.inputs.allcostx is not working
# record final registration costs
allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
allineate.inputs.allcostx = 'out.allcostX.txt'
allineate.inputs.reference = 'final_epi_vr_base_min_outlier+tlrc.BRIK'
allineate.inputs.in_file = 'anat_final.{}+tlrc.BRIK'.format(sub)
allineate.cmdline
# should be
# 3dAllineate -base final_epi_vr_base_min_outlier+tlrc -allcostX  \
#             -input anat_final.$subj+tlrc |& tee out.allcostX.txt
res = allineate.run()
"""
!3dAllineate -base final_epi_vr_base_min_outlier+tlrc -allcostX  \
            -input anat_final.FT+tlrc |& tee out.allcostX.txt

compare2text(3, opj(path_afni_preprocessed_files, 'out.allcostX.txt'), 'out.allcostX.txt')



#####################################
#####################################
# part factorized
######################################
######################################

# --------------------------------------
# selectfile
# String template with {}-based strings
templates = {'pb02': abspath('pb02.FT.r01.volreg+tlrc.BRIK'),
             'maskepi': abspath('mask_epi_extents+tlrc.BRIK.gz')}
# Create SelectFiles node
sf = Node(SelectFiles(templates),
          name='selectfiles')

# Location of the dataset folder
sf.inputs.base_directory = '.'


# create a TSNR dataset, just from run 1
tstat = Node(afni.TStat(), name='tstat') # Compute voxel-wise statistics
tstat.inputs.args = '-mean'
tstat.inputs.out_file = 'rm.signal.vreg.r01+tlrc.BRIK'

detrend = Node(afni.Detrend(), name='detrend') # Compute voxel-wise statistics
detrend.inputs.args = '-polort 3 -overwrite'
detrend.inputs.outputtype = 'AFNI'
detrend.inputs.out_file = 'rm.noise.det+tlrc.BRIK'

tstat2 = Node(afni.TStat(), name='tstat2') # Compute voxel-wise statistics
tstat2.inputs.args = '-stdev'
tstat2.inputs.out_file = 'rm.noise.vreg.r01+tlrc.BRIK'

calc = Node(afni.Calc(), name='calc') # This program does voxel-by-voxel arithmetic on 3D datasets.
calc.inputs.expr='c*a/b'
calc.inputs.out_file =  'TSNR.vreg.r01.FT+tlrc.BRIK'


####### build workflow
wf_run = Workflow(name="CreateTSNRdataset")
wf_run.base_dir = '.'
wf_run.connect([(sf, tstat, [("pb02", "in_file")]),
                (sf, detrend, [("pb02", "in_file")]),
                (detrend, tstat2, [("out_file", "in_file")]),
                (tstat, calc, [("out_file", "in_file_a")]),
                (tstat2, calc, [("out_file", "in_file_b")]),
                (sf, calc, [("maskepi", "in_file_c")])
                ])
wf_run.write_graph(graph2use='exec')
wf_run.write_graph(graph2use='hierarchical')
wf_run.run()

compare2brik(opj(path_afni_preprocessed_files, "TSNR.vreg.r01.FT+tlrc.BRIK"),"/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/CreateTSNRdataset/calc/TSNR.vreg.r01.FT+tlrc.BRIK") # TRUE

#####################################
#####################################
# part NOT factorized
######################################
######################################


################################
####### BLOCK EXPLANATION ######
"""
1. align/warp with FT_anat+orig.BRIK with output same grid as anat_final.{}+tlrc.BRIK
    save output as anat_w_skull_warped+tlrc.BRIK
2. compute correlations with spherical ~averages pb02.$subj.r*.volreg
    outputs stored in radcor
"""
##################################
##################################

# -----------------------------------------
# warp anat follower datasets (affine)
# warp follower dataset FT_anat+orig
allineate = afni.Allineate() # Program to align one dataset (the ‘source’) to a base dataset
allineate.inputs.in_file = 'FT_anat+orig.BRIK'
allineate.inputs.master = 'anat_final.{}+tlrc.BRIK'.format(sub)
allineate.inputs.final_interpolation = 'wsinc5'
allineate.inputs.in_matrix = 'warp.anat.Xat.1D'
allineate.inputs.out_file = 'anat_w_skull_warped+tlrc.BRIK'
allineate.cmdline
# should be
# 3dAllineate -source FT_anat+orig                                \
#             -master anat_final.$subj+tlrc                       \
#             -final wsinc5 -1Dmatrix_apply warp.anat.Xat.1D      \
#             -prefix anat_w_skull_warped
res = allineate.run()

compare2brik(opj(path_afni_preprocessed_files, "anat_w_skull_warped+tlrc.BRIK"),"anat_w_skull_warped+tlrc.BRIK") # TRUE



# ---------------------------------------------------------

"""missing
# QC: compute correlations with spherical ~averages
@radial_correlate -nfirst 0 -polort 3 -do_clean yes             \
                  -rdir radcor.pb02.volreg                      \
                  pb02.$subj.r*.volreg+tlrc.HEAD
"""
# not working in nipype thus copying output directories from afni tutorial with afni
src = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_afni/FT.results/radcor.pb02.volreg"
dst = "/home/jlefortb/reproduction_afni_tutorial/afni_tuto_w_nipype/FT.results/radcor.pb02.volreg"
shutil.copytree(src, dst)



# ================================== blur ==================================

################################
####### BLOCK EXPLANATION######
"""
1. for each run
1.1 Gaussian blur with FWHM =4 of pb02.{}.r0{}.volreg+tlrc.BRIK
        save in pb03.{}.r0{}.blur+tlrc.BRIK
2. for each run
2.1 create 'full_mask' dataset (union mask) from pb03.{}.r0{}.blur+tlrc.BRIK
        save in rm.mask_r0{}+tlrc.BRIK.gz
            Input dataset is EPI 3D+time, or a skull-stripped anatomical.
            Output dataset is a brain-only mask dataset.
3.combine 3 rm.mask_r0{}+tlrc.BRIK.gz into one mask (union)
    save in full_mask.{}+tlrc.BRIK.gz
4. create subject anatomy mask, mask_anat.$subj+tlrc (resampled from tlrc anat)
4.1 Resample FT_anat_ns+tlrc.BRIK with full_mask
    save in rm.resam.anat+tlrc.BRIK
4.2 convert to binary anat mask; fill gaps and holes
    from rm.resam.anat+tlrc.BRIK, dilate and then erode, which connects areas that are close
    to mask_anat.{}+tlrc.BRIK.gz
4.3 compute tighter EPI mask by intersecting with anat mask
        from full_mask to mask_epi_anat.{}+tlrc.BRIK.gz
4.4 compute overlaps between anat and EPI masks
        Output (to screen) is a count of various things about how
        the automasks of datasets A and B overlap or don't overlap
        b/w full_mask and mask_anat
        save in out.mask_ae_overlap.txt
4.5 note Dice coefficient of masks, as well
        compute Correlation coefficient between sub-brick pairs
        b/w full_mask and mask_anat 
        save in out.mask_ae_dice.txt

"""
##################################
##################################


#####################################
#####################################
# part factorized
######################################
######################################

# selectfile
# String template with {}-based strings
templates = {'for_resample': abspath('FT_anat_ns+tlrc.BRIK')}
# Create SelectFiles node
sf = Node(SelectFiles(templates),
          name='selectfiles')
# Location of the dataset folder
sf.inputs.base_directory = '.'

infosource = Node(IdentityInterface(fields=['run_nb']),
                  name="infosource")
infosource.iterables = [('to_merge', [abspath('pb02.FT.r0{}.volreg+tlrc.BRIK'.format(run)) for run in runs])]

# blur each volume of each run
merge = Node(afni.Merge(), name='merge')  # Merge or edit volumes using AFNI 3dmerge command
merge.inputs.blurfwhm = 4
merge.inputs.doall = True
merge.inputs.out_file = 'pb03.FT.blur+tlrc.BRIK'


automask = Node(afni.Automask(), name='automask') # Create a brain-only mask of the image using AFNI 3dAutomask command
automask.inputs.brain_file = "skullstripped_pb03_r0{}+tlrc.BRIK".format(run)
automask.inputs.out_file = "rm.mask+tlrc.BRIK.gz"

# create union of inputs, output type is byte
masktool = Node(afni.MaskTool(),name='masktool') # for combining/dilating/eroding/filling masks
masktool.inputs.in_file = ['rm.mask+tlrc.BRIK.gz' for run in runs]
masktool.inputs.out_file = "full_mask.FT+tlrc.BRIK.gz"
masktool.inputs.union = True


# ---- create subject anatomy mask, mask_anat.$subj+tlrc ----
#      (resampled from tlrc anat)
resample = Node(afni.Resample(), name='resample') # Resample or reorient an image using AFNI 3dresample command
resample.inputs.out_file = 'rm.resam.anat+tlrc.BRIK'


# convert to binary anat mask; fill gaps and holes
masktool2 = Node(afni.MaskTool(),name='masktool2') # for combining/dilating/eroding/filling masks
masktool2.inputs.dilate_inputs = '5 -5'
masktool2.inputs.fill_holes = True
masktool2.inputs.out_file = "mask_anat.FT+tlrc.BRIK.gz"


# compute tighter EPI mask by intersecting with anat mask
masktool3 = Node(afni.MaskTool(),name='masktool3') # for combining/dilating/eroding/filling masks
masktool3.inputs.out_file = "mask_epi_anat.FT+tlrc.BRIK.gz"
masktool3.inputs.inter = True


# compute overlaps between anat and EPI masks
aboverlap = Node(afni.ABoverlap(), name='aboverlap')
aboverlap.inputs.no_automask = True
aboverlap.inputs.out_file =  'out.mask_ae_overlap.txt'

####### build workflow
wf_run = Workflow(name="BlurAndMask")
wf_run.base_dir = '.'
wf_run.connect([(infosource, merge, [("to_merge", "in_files")]),
                (merge, automask, [("out_file", "in_file")]),
                (detrend, tstat2, [("out_file", "in_file")]),
                (tstat, calc, [("out_file", "in_file_a")]),
                (tstat2, calc, [("out_file", "in_file_b")]),
                (sf, calc, [("maskepi", "in_file_c")])
                ])
wf_run.write_graph(graph2use='exec')
wf_run.write_graph(graph2use='hierarchical')
wf_run.run()


#####################################
#####################################
# TO DO: finir le connect du workflow BlurAndMak
# part not YET factorized
######################################
######################################


# blur each volume of each run
for run in runs:
    merge = afni.Merge() # Merge or edit volumes using AFNI 3dmerge command
    merge.inputs.in_files = 'pb02.{}.r0{}.volreg+tlrc.BRIK'.format(sub, run)
    merge.inputs.blurfwhm = 4
    merge.inputs.doall = True
    merge.inputs.out_file = 'pb03.{}.r0{}.blur+tlrc.BRIK'.format(sub, run)
    print(merge.cmdline)
    # should be:
    # 3dmerge -1blur_fwhm 4.0 -doall -prefix pb03.$subj.r$run.blur \
    #         pb02.$subj.r$run.volreg+tlrc
    res = merge.run() 

    # ================================== mask ==================================
    # create 'full_mask' dataset (union mask)

    automask = afni.Automask() # Create a brain-only mask of the image using AFNI 3dAutomask command
    automask.inputs.in_file = 'pb03.{}.r0{}.blur+tlrc.BRIK'.format(sub, run)
    automask.inputs.brain_file = "skullstripped_pb03_r0{}+tlrc.BRIK".format(run)
    automask.inputs.out_file = "rm.mask_r0{}+tlrc.BRIK.gz".format(run)
    automask.cmdline  # doctest: +ELLIPSIS
    # should be:
    # 3dAutomask -prefix rm.mask_r$run pb03.$subj.r$run.blur+tlrc
    res = automask.run()  # doctest: +SKIP

for run in runs:
    compare2brik(opj(path_afni_preprocessed_files, "pb03.FT.r0{}.blur+tlrc.BRIK".format(run)),"pb03.FT.r0{}.blur+tlrc.BRIK".format(run)) # TRUE

# create union of inputs, output type is byte
masktool = afni.MaskTool() # for combining/dilating/eroding/filling masks
masktool.inputs.in_file = ['rm.mask_r0{}+tlrc.BRIK.gz'.format(run) for run in runs]
masktool.inputs.out_file = "full_mask.{}+tlrc.BRIK.gz".format(sub)
masktool.inputs.union = True
masktool.cmdline
# should be:
# 3dmask_tool -inputs rm.mask_r*+tlrc.HEAD -union -prefix full_mask.$subj
res = masktool.run() 
compare2brik(opj(path_afni_preprocessed_files, "full_mask.{}+tlrc.BRIK.gz".format(sub)),"full_mask.{}+tlrc.BRIK.gz".format(sub)) # TRUE



# ---- create subject anatomy mask, mask_anat.$subj+tlrc ----
#      (resampled from tlrc anat)
resample = afni.Resample() # Resample or reorient an image using AFNI 3dresample command
resample.inputs.in_file = 'FT_anat_ns+tlrc.BRIK'
resample.inputs.master = 'full_mask.{}+tlrc.BRIK.gz'.format(sub)
resample.inputs.out_file = 'rm.resam.anat+tlrc.BRIK'
resample.cmdline
# Should be:
# 3dresample -master full_mask.$subj+tlrc -input FT_anat_ns+tlrc        \
#            -prefix rm.resam.anat
res = resample.run()  # doctest: +SKIP


# convert to binary anat mask; fill gaps and holes
masktool = afni.MaskTool() # for combining/dilating/eroding/filling masks
masktool.inputs.in_file = 'rm.resam.anat+tlrc.BRIK'
masktool.inputs.dilate_inputs = '5 -5'
masktool.inputs.fill_holes = True
masktool.inputs.out_file = "mask_anat.{}+tlrc.BRIK.gz".format(sub)
masktool.cmdline
# should be:
# 3dmask_tool -dilate_input 5 -5 -fill_holes -input rm.resam.anat+tlrc  \
#             -prefix mask_anat.$subj
res = masktool.run() 
compare2brik(opj(path_afni_preprocessed_files, "mask_anat.{}+tlrc.BRIK.gz".format(sub)),"mask_anat.{}+tlrc.BRIK.gz".format(sub)) # TRUE



# compute tighter EPI mask by intersecting with anat mask
masktool = afni.MaskTool() # for combining/dilating/eroding/filling masks
masktool.inputs.in_file = 'full_mask.{}+tlrc.BRIK.gz'.format(sub)
masktool.inputs.out_file = "mask_epi_anat.{}+tlrc.BRIK.gz".format(sub)
masktool.inputs.inter = True
masktool.cmdline
# should be:
# 3dmask_tool -input full_mask.$subj+tlrc mask_anat.$subj+tlrc          \
#             -inter -prefix mask_epi_anat.$subj
res = masktool.run() 
compare2brik(opj(path_afni_preprocessed_files, "mask_epi_anat.{}+tlrc.BRIK.gz".format(sub)),"mask_epi_anat.{}+tlrc.BRIK.gz".format(sub)) # TRUE
# results are different

# compute overlaps between anat and EPI masks
aboverlap = afni.ABoverlap()
aboverlap.inputs.in_file_a = 'full_mask.{}+tlrc.BRIK.gz'.format(sub)
aboverlap.inputs.in_file_b = 'mask_anat.{}+tlrc.BRIK.gz'.format(sub)
aboverlap.inputs.no_automask = True
aboverlap.inputs.out_file =  'out.mask_ae_overlap.txt'
aboverlap.cmdline
# should be
# 3dABoverlap -no_automask full_mask.$subj+tlrc mask_anat.$subj+tlrc    \
#             |& tee out.mask_ae_overlap.txt
res = aboverlap.run() 
compare2text(3, opj(path_afni_preprocessed_files, "out.mask_ae_overlap.txt"),"out.mask_ae_overlap.txt") # TRUE


"""ERROR with 3dDot instead of 3ddot in nipype + error on website
# note Dice coefficient of masks, as well
dot = afni.Dot() # Correlation coefficient between sub-brick pairs. All datasets in in_files list will be concatenated. 
dot.inputs.in_files = ['full_mask.{}+tlrc.BRIK.gz'.format(sub), 'mask_anat.{}+tlrc.BRIK.gz'.format(sub)]
dot.inputs.dodice = True
dot.inputs.out_file = 'out.mask_ae_dice.txt'
dot.cmdline
# should be
# 3ddot -dodice full_mask.$subj+tlrc mask_anat.$subj+tlrc               \
#       |& tee out.mask_ae_dice.txt
res = dot.run()  
"""
!3ddot -dodice full_mask.FT+tlrc mask_anat.FT+tlrc               \
      |& tee out.mask_ae_dice.txt
compare2text(3, opj(path_afni_preprocessed_files, "out.mask_ae_dice.txt"),"out.mask_ae_dice.txt") # TRUE







# ---- create group anatomy mask, mask_group+tlrc ----
#      (resampled from tlrc base anat, TT_N27+tlrc)
resample = afni.Resample() # Resample or reorient an image using AFNI 3dresample command
resample.inputs.in_file = '/home/jlefortb/abin/TT_N27+tlrc.BRIK.gz'
resample.inputs.master = 'full_mask.{}+tlrc.BRIK.gz'.format(sub)
resample.inputs.out_file = 'rm.resam.group+tlrc.BRIK'
resample.cmdline
# Should be:
# 3dresample -master full_mask.$subj+tlrc -prefix ./rm.resam.group      \
#            -input /home/jlefortb/abin/TT_N27+tlrc
res = resample.run()  # doctest: +SKIP


# convert to binary group mask; fill gaps and holes
masktool = afni.MaskTool() # for combining/dilating/eroding/filling masks
masktool.inputs.in_file = 'rm.resam.group+tlrc.BRIK'
masktool.inputs.out_file = 'mask_group+tlrc.BRIK.gz'
masktool.inputs.dilate_inputs = "5 -5"
masktool.inputs.fill_holes = True
masktool.cmdline
# should be:
# 3dmask_tool -dilate_input 5 -5 -fill_holes -input rm.resam.group+tlrc \
#             -prefix mask_group
res = masktool.run() 
compare2brik(opj(path_afni_preprocessed_files, "mask_group+tlrc.BRIK.gz"),"mask_group+tlrc.BRIK.gz") # TRUE

"""ERROR with 3dDot instead of 3ddot in nipype + error on website
# note Dice coefficient of anat and template masks
dot = afni.Dot() # Correlation coefficient between sub-brick pairs. All datasets in in_files list will be concatenated. 
dot.inputs.in_files = ['mask_anat.{}+tlrc.BRIK.gz'.format(sub), 'mask_group+tlrc.BRIK.gz']
dot.inputs.dodice = True
dot.inputs.out_file = 'out.mask_at_dice.txt'
dot.cmdline
# should be
# 3ddot -dodice mask_anat.$subj+tlrc mask_group+tlrc                    \
#       |& tee out.mask_at_dice.txt
res = dot.run() 
"""
!3ddot -dodice mask_anat.FT+tlrc mask_group+tlrc                    \
      |& tee out.mask_at_dice.txt
compare2text(3, opj(path_afni_preprocessed_files, "out.mask_at_dice.txt"),"out.mask_at_dice.txt") # TRUE


# ================================= scale ==================================

################################
####### BLOCK EXPLANATION ######
"""
1. for each run
1.1 computes the mean of each voxel time series
    for pb03.{}.r0{}.blur+tlrc.BRIK
    save in rm.mean_r0{}+tlrc.BRIK
1.2 compute :
    mask_epi_extents * min(200, pb03.{}.r0{}.blur / rm.mean_r0{}*100)*step(pb03.{}.r0{}.blur)*step(rm.mean_r0{})
    save in pb04.{}.r0{}.scale

"""
##################################
##################################


# scale each voxel time series to have a mean of 100
# (be sure no negatives creep in)
# (subject to a range of [0,200])
for run in runs:
    tstat = afni.TStat() # Compute voxel-wise statistics
    tstat.inputs.in_file = 'pb03.{}.r0{}.blur+tlrc.BRIK'.format(sub, run)
    tstat.inputs.out_file = 'rm.mean_r0{}+tlrc.BRIK'.format(run)
    print(tstat.cmdline)
    # should be:
    # 3dTstat -prefix rm.mean_r$run pb03.$subj.r$run.blur+tlrc
    res = tstat.run()

    calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
    calc.inputs.in_file_a = 'pb03.{}.r0{}.blur+tlrc.BRIK'.format(sub, run)
    calc.inputs.in_file_b = 'rm.mean_r0{}+tlrc.BRIK'.format(run)
    calc.inputs.in_file_c = 'mask_epi_extents+tlrc.BRIK.gz'
    calc.inputs.expr = 'c * min(200, a/b*100)*step(a)*step(b)'
    calc.inputs.out_file = 'pb04.{}.r0{}.scale+tlrc.BRIK'.format(sub, run)
    print(calc.cmdline)
    # should be:
    # 3dcalc -a pb03.$subj.r$run.blur+tlrc -b rm.mean_r$run+tlrc \
    #        -c mask_epi_extents+tlrc                            \
    #        -expr 'c * min(200, a/b*100)*step(a)*step(b)'       \
    #        -prefix pb04.$subj.r$run.scale
    res = calc.run()
compare2brik(opj(path_afni_preprocessed_files, 'pb04.{}.r0{}.scale+tlrc.BRIK'.format(sub, run)),'pb04.{}.r0{}.scale+tlrc.BRIK'.format(sub, run)) # TRUE


# ================================ regress =================================

################################
####### BLOCK EXPLANATION ######
"""
1. compute de-meaned motion parameters (for use in regression)
    form dfile_rall to motion_dmean.1D
2. compute motion parameter derivatives
    form dfile_rall to motion_deriv.1D
3. convert motion parameters for per-run regression
    split motion_dmean.1D into 3 files : mot_demean.runNumber
4. create censor file motion_${subj}_censor.1D, for censoring motion
5. combine multiple censor files, motion_{}_censor.1D * outcount_{}_censor.1D
   save in censor_{}_combined_2.1D
6. note TRs that were not censored and save into variable 'ktrs'
"""
##################################
##################################

# compute de-meaned motion parameters (for use in regression)
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets. 
odt.inputs.in_file = 'dfile_rall.1D'
odt.inputs.set_nruns = 3
odt.inputs.demean = True
odt.inputs.out_file = 'motion_dmean.1D'
odt.cmdline
# should be:
# 1d_tool.py -infile dfile_rall.1D -set_nruns 3                            \
#            -demean -write motion_demean.1D
res = odt.run() 


# compute motion parameter derivatives (just to have)
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets. 
odt.inputs.in_file = 'dfile_rall.1D'
odt.inputs.set_nruns = 3
odt.inputs.demean = True
odt.inputs.derivative = True
odt.inputs.out_file = 'motion_deriv.1D'
odt.cmdline
# should be:
# 1d_tool.py -infile dfile_rall.1D -set_nruns 3                            \
#            -derivative -demean -write motion_deriv.1D
res = odt.run() 

# convert motion parameters for per-run regression
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets. 
odt.inputs.in_file = 'motion_dmean.1D'
odt.inputs.set_nruns = 3
odt.inputs.args = "-split_into_pad_runs mot_demean"
odt.cmdline
# should be:
# 1d_tool.py -infile motion_demean.1D -set_nruns 3                         \
#            -split_into_pad_runs mot_demean
res = odt.run() 
compare2text(3, opj(path_afni_preprocessed_files, 'mot_demean.r02.1D'),'mot_demean.r02.1D') # TRUE


# create censor file motion_${subj}_censor.1D, for censoring motion
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets. 
odt.inputs.in_file = 'dfile_rall.1D'
odt.inputs.set_nruns = 3
odt.inputs.censor_motion = (0.3, 'motion_{}'.format(sub))
odt.inputs.censor_prev_TR = True
odt.inputs.show_censor_count = True
odt.cmdline
# should be:
# 1d_tool.py -infile dfile_rall.1D -set_nruns 3                            \
#     -show_censor_count -censor_prev_TR                                   \
#     -censor_motion 0.3 motion_${subj}
res = odt.run() 
compare2text(3, opj(path_afni_preprocessed_files, 'motion_FT_enorm.1D'),'motion_FT_enorm.1D') # TRUE
compare2text(3, opj(path_afni_preprocessed_files, 'motion_FT_CENSORTR.txt'),'motion_FT_CENSORTR.txt') # TRUE


""" ERROR : stderr 2023-02-15T11:31:06.184859:** -prefix = unknown command line option!
# combine multiple censor files
Path('censor_{}_combined_2.1D'.format(sub)).touch() # start with empty file
eval = afni.Eval()# Evaluates an expression that may include columns of data from one or more text files
eval.inputs.in_file_a = 'motion_{}_censor.1D'.format(sub)
eval.inputs.in_file_b = 'outcount_{}_censor.1D'.format(sub)
eval.inputs.expr = "a*b"
eval.inputs.out_file =  'censor_{}_combined_2.1D'.format(sub)
eval.cmdline
# should be:
# 1deval -a motion_${subj}_censor.1D -b outcount_${subj}_censor.1D         \
#        -expr "a*b" > censor_${subj}_combined_2.1D
res = eval.run() 
compare2text(3, opj(path_afni_preprocessed_files, 'censor_{}_combined_2.1D'.format(sub)),'censor_{}_combined_2.1D'.format(sub)) # TRUE
"""
!1deval -a motion_FT_censor.1D -b outcount_FT_censor.1D         \
        -expr "a*b" > censor_FT_combined_2.1D
compare2text(3, opj(path_afni_preprocessed_files, 'censor_{}_combined_2.1D'.format(sub)),'censor_{}_combined_2.1D'.format(sub)) # TRUE



# note TRs that were not censored
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets. 
odt.inputs.in_file = 'censor_{}_combined_2.1D'.format(sub)
odt.inputs.show_trs_uncensored = 'encoded'
odt.cmdline
# should be:
# 1d_tool.py -infile dfile_rall.1D -set_nruns 3                            \
#     -show_censor_count -censor_prev_TR                                   \
#     -censor_motion 0.3 motion_${subj}
res = odt.run() 
ktrs = !1d_tool.py -infile censor_FT_combined_2.1D              \
                       -show_trs_uncensored encoded
# ['0..40,45..264,267..449']


################################
####### BLOCK EXPLANATION ######
"""
1. run the regression analysis
1.1 inputs are : 
        -pb04.{}.r0{}.scale+tlrc for each run
        -the censor files
        -the stimulus time and labels
1.2 the contrasts are:
        -('SYM: vis -aud')
        -and ('SYM: 0.5*vis +0.5*aud')
1.3 main outputs save in 
        -stats.{}+tlrc.BRIK
        -X.jpg (contrast map)

"""
##################################
##################################

# ------------------------------
# run the regression analysis
deconvolve = afni.Deconvolve() # Performs OLS regression given a 4D neuroimage file and stimulus timings
deconvolve.inputs.in_files = ['pb04.{}.r0{}.scale+tlrc.HEAD'.format(sub, run) for run in runs]
deconvolve.inputs.censor = 'censor_{}_combined_2.1D'.format(sub)
# deconvolve.inputs.ortvec = [('mot_demean.r01.1D', 'mot_demean_r01')
# deconvolve.inputs.ortvec = ('mot_demean.r02.1D', 'mot_demean_r02')
# deconvolve.inputs.ortvec = ('mot_demean.r03.1D', 'mot_demean_r03')
deconvolve.inputs.polort = 3
deconvolve.inputs.num_stimts = 2
deconvolve.inputs.stim_times = [(1, "stimuli/AV1_vis.txt", 'BLOCK(20,1)'), (2, "stimuli/AV2_aud.txt", 'BLOCK(20,1)')]
deconvolve.inputs.stim_label = [(1, "vis"), (2, 'aud')]
deconvolve.inputs.num_threads = 2
deconvolve.inputs.fout =True 
deconvolve.inputs.tout = True 
deconvolve.inputs.gltsym = [('SYM: vis -aud'), ('SYM: 0.5*vis +0.5*aud')]
deconvolve.inputs.glt_label = [(1, 'V-A'), (2, 'mean.VA')]
deconvolve.inputs.x1D = "X.xmat.1D"
deconvolve.inputs.args = "-xjpeg X.jpg -errts errts.{} -x1D_uncensored X.nocensor.xmat.1D -ortvec mot_demean.r01.1D mot_demean_r01 -ortvec mot_demean.r02.1D mot_demean_r02 -ortvec mot_demean.r03.1D mot_demean_r03".format(sub)
deconvolve.inputs.out_file = 'stats.{}+tlrc.BRIK'.format(sub)
deconvolve.cmdline
# should be
# 3dDeconvolve -input pb04.$subj.r*.scale+tlrc.HEAD                        \
#     -censor censor_${subj}_combined_2.1D                                 \
#     -ortvec mot_demean.r01.1D mot_demean_r01                             \
#     -ortvec mot_demean.r02.1D mot_demean_r02                             \
#     -ortvec mot_demean.r03.1D mot_demean_r03                             \
#     -polort 3                                                            \
#     -num_stimts 2                                                        \
#     -stim_times 1 stimuli/AV1_vis.txt 'BLOCK(20,1)'                      \
#     -stim_label 1 vis                                                    \
#     -stim_times 2 stimuli/AV2_aud.txt 'BLOCK(20,1)'                      \
#     -stim_label 2 aud                                                    \
#     -jobs 2                                                              \
#     -gltsym 'SYM: vis -aud'                                              \
#     -glt_label 1 V-A                                                     \
#     -gltsym 'SYM: 0.5*vis +0.5*aud'                                      \
#     -glt_label 2 mean.VA                                                 \
#     -fout -tout -x1D X.xmat.1D -xjpeg X.jpg                              \
#     -x1D_uncensored X.nocensor.xmat.1D                                   \
#     -errts errts.${subj}                                                 \
#     -bucket stats.$subj
res = deconvolve.run()  # doctest: +SKIP

compare2text(3, opj(path_afni_preprocessed_files, 'X.xmat.1D'),'X.xmat.1D') # TRUE
compare2brik(opj(path_afni_preprocessed_files, 'stats.FT+tlrc.BRIK'),'stats.FT+tlrc.BRIK') # TRUE
compare2brik(opj(path_afni_preprocessed_files, 'errts.FT+tlrc.BRIK'),'errts.FT+tlrc.BRIK') # TRUE

################################
####### BLOCK EXPLANATION ######
"""
1. display any large pairwise correlations from the X-matrix
        from 'X.xmat.1D' to "out.cormat_warn.txt"
2. display degrees of freedom info from X-matrix
        from 'X.xmat.1D' to "out.df_info.txt"
3. create an all_runs dataset to match the fitts, errts, etc..
        from pb04.{}.r0{}.scale for each run
        to "all_runs.{}+tlrc
4. create a temporal signal to noise ratio dataset 
       signal: if 'scale' block, mean should be 100
       noise : compute standard deviation of errts
    from all_runs.{}+tlrc (subselection of ktrs)
    to rm.signal.all
    And from 'errts.{}+tlrc (subselection of ktrs)
    to rm.noise.all
5. compute rm.signal.all * rm.noise.all
    save in 'TSNR.{}+tlrc.BRIK

"""
##################################
##################################

# display any large pairwise correlations from the X-matrix
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
odt.inputs.in_file = 'X.xmat.1D'
odt.inputs.show_cormat_warnings = "out.cormat_warn.txt"
odt.cmdline
# should be:
# 1d_tool.py -show_cormat_warnings -infile X.xmat.1D |& tee out.cormat_warn.txt
res = odt.run() 
compare2text(3, opj(path_afni_preprocessed_files, 'out.cormat_warn.txt'),'out.cormat_warn.txt') # TRUE



# display degrees of freedom info from X-matrix
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
odt.inputs.in_file = 'X.xmat.1D'
odt.inputs.args = "-show_df_info"
odt.inputs.show_cormat_warnings = "out.df_info.txt"
odt.cmdline
# should be:
# 1d_tool.py -show_df_info -infile X.xmat.1D |& tee out.df_info.txt
res = odt.run() 
compare2text(3, opj(path_afni_preprocessed_files, 'out.df_info.txt'),'out.df_info.txt') # TRUE



# create an all_runs dataset to match the fitts, errts, etc.
tcat = afni.TCat() # Concatenate sub-bricks from input datasets into one big 3D+time dataset.
tcat.inputs.in_files = ['pb04.{}.r0{}.scale+tlrc.HEAD'.format(sub, run) for run in runs]
tcat.inputs.out_file = "all_runs.{}+tlrc.BRIK".format(sub)
tcat.cmdline
# should be
# 3dTcat -prefix all_runs.$subj pb04.$subj.r*.scale+tlrc.HEAD
res = tcat.run()  
compare2brik(opj(path_afni_preprocessed_files, "all_runs.{}+tlrc.BRIK".format(sub)),"all_runs.{}+tlrc.BRIK".format(sub)) # TRUE


# --------------------------------------------------
# create a temporal signal to noise ratio dataset 
#    signal: if 'scale' block, mean should be 100
#    noise : compute standard deviation of errts
"""ERROR - mean is not coded in nipype
tcsb = afni.TCatSubBrick() # allow sub-brick selection 
tcsb.inputs.in_files = [('all_runs.{}+tlrc.BRIK'.format(sub), "\"[{}]\"".format(ktrs[0]))]
tcsb.inputs.out_file = "rm.signal.all"
tcsb.inputs.args = "-mean"
tcsb.cmdline
# should be
# 3dTstat -mean -prefix rm.signal.all all_runs.$subj+tlrc"[$ktrs]"
res = tcsb.run() 
"""
!3dTstat -mean -prefix rm.signal.all all_runs.FT+tlrc"[0..40,45..264,267..449]"

"""ERROR - stdev is not coded in nipype
tcsb = afni.TCatSubBrick() # allow sub-brick selection 
tcsb.inputs.in_files = [('errts.{}+tlrc.BRIK'.format(sub), "\"[{}]\"".format(ktrs[0]))]
tcsb.inputs.out_file = "rm.noise.all"
tcsb.inputs.args = "-stdev"
tcsb.cmdline
# should be
# 3dTstat -stdev -prefix rm.noise.all errts.${subj}+tlrc"[$ktrs]"
res = tcsb.run() 
"""
!3dTstat -stdev -prefix rm.noise.all errts.FT+tlrc"[0..40,45..264,267..449]"

calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
calc.inputs.in_file_a = 'rm.signal.all+tlrc.BRIK'
calc.inputs.in_file_b = 'rm.noise.all+tlrc.BRIK'
calc.inputs.expr = 'a/b'
calc.inputs.out_file = 'TSNR.{}+tlrc.BRIK'.format(sub)
calc.cmdline
# should be:
# 3dcalc -a rm.signal.all+tlrc                                             \
#        -b rm.noise.all+tlrc                                              \
#        -expr 'a/b' -prefix TSNR.$subj
res = calc.run()
compare2brik(opj(path_afni_preprocessed_files, 'TSNR.{}+tlrc.BRIK'.format(sub)),'TSNR.{}+tlrc.BRIK'.format(sub)) # TRUE



################################
####### BLOCK EXPLANATION ######
"""
1. compute and store GCOR (global correlation average)
     (sum of squares of global mean of unit errts)
1.1 Shifts voxel time series from errts.{}+tlrc so that seperate slices are aligned to the same temporal origin.
    save in 'rm.errts.unit+tlrc.BRIK'
1.2 Computes average of all voxels in the rm.errts.unit which satisfy the full_mask
    save in mean.errts.unit.1D
1.3 compute sum of squares mean.errts
        save in out.gcor.1D
2. compute correlation volume
     (per voxel: correlation with masked brain average)
    Computes average of all voxels in errts.{}+tlrc which satisfy the full_mask
    save in "mean.errts.1D"
3. Computes the correlation coefficient between each voxel time series 
    in the errts.{}+tlrc.
    save in corr_brain+tlrc
4. create fitts dataset from all_runs and errts
    compute all_runs.{}+tlrc - errts.{}+tlrc
    save in 'fitts.{}+tlrc.BRIK'.format(sub)
5. create ideal files for fixed response stim types
        1dcat takes as input one or more 1D files, and writes out a 1D file 
         containing the side-by-side concatenation of all or a subset of the columns from the input files.
    Here, from X.nocensor.xmat.1D to ideal_vis.1D, and again to ideal_aud.1D
6. extract non-baseline regressors from the X-matrix,
     then compute their sum
    from X.nocensor.xmat.1D' to X.stim.xmat.1D"
7 compute sum voxel-wise of X.stim.xmat.1D 
    save in sum_ideal.1D

"""
##################################
##################################


# ---------------------------------------------------
# compute and store GCOR (global correlation average)
# (sum of squares of global mean of unit errts)
tnorm = afni.TNorm() # Shifts voxel time series from input so that seperate slices are aligned to the same temporal origin.
tnorm.inputs.in_file = 'errts.{}+tlrc.BRIK'.format(sub)
tnorm.inputs.norm2 = True
tnorm.inputs.out_file = 'rm.errts.unit+tlrc.BRIK'.format(sub)
tnorm.cmdline
# should be
# 3dTnorm -norm2 -prefix rm.errts.unit errts.${subj}+tlrc
res = tnorm.run()  

maskave = afni.Maskave() # Computes average of all voxels in the input dataset which satisfy the criterion in the options list
maskave.inputs.in_file = 'rm.errts.unit+tlrc.BRIK'
maskave.inputs.mask= 'full_mask.{}+tlrc.BRIK.gz'.format(sub)
maskave.inputs.quiet= True
maskave.inputs.out_file = "mean.errts.unit.1D"
maskave.cmdline 
# should be 
# 3dmaskave -quiet -mask full_mask.$subj+tlrc rm.errts.unit+tlrc           \
#          > mean.errts.unit.1D
res = maskave.run()  
compare2text(3, opj(path_afni_preprocessed_files, 'mean.errts.unit.1D'),'mean.errts.unit.1D') # TRUE

"""ERROR with input file (must be transposed)
tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_file = 'mean.errts.unit.1D'
tstat.inputs.args = '-sos'
tstat.inputs.out_file = "mean.errts.1D"
tstat.cmdline
# should be
# 3dTstat -sos -prefix - mean.errts.unit.1D\' > out.gcor.1D
outgcor = tstat.run()
"""
!3dTstat -sos -prefix - mean.errts.unit.1D\' > out.gcor.1D

print("-- GCOR = `cat out.gcor.1D`")
compare2text(3, opj(path_afni_preprocessed_files, 'out.gcor.1D'),'out.gcor.1D') # TRUE


# ---------------------------------------------------
# compute correlation volume
# (per voxel: correlation with masked brain average)
maskave = afni.Maskave() # Computes average of all voxels in the input dataset which satisfy the criterion in the options list
maskave.inputs.in_file = 'errts.{}+tlrc.BRIK'.format(sub)
maskave.inputs.mask = 'full_mask.{}+tlrc.BRIK.gz'.format(sub)
maskave.inputs.quiet = True
maskave.inputs.out_file = "mean.errts.1D"
maskave.cmdline 
# should be 
# 3dmaskave -quiet -mask full_mask.$subj+tlrc errts.${subj}+tlrc           \
#           > mean.errts.1D
res = maskave.run()  
compare2text(3, opj(path_afni_preprocessed_files, 'mean.errts.1D'),'mean.errts.1D') # TRUE


tcorr1D = afni.TCorr1D() # Computes the correlation coefficient between each voxel time series in the input 3D+time dataset.
tcorr1D.inputs.xset= 'errts.{}+tlrc.BRIK'.format(sub)
tcorr1D.inputs.y_1d = 'mean.errts.1D'
tcorr1D.inputs.out_file = 'corr_brain+tlrc.BRIK'
tcorr1D.cmdline
# should be
# 3dTcorr1D -prefix corr_brain errts.${subj}+tlrc mean.errts.1D
res = tcorr1D.run() 
compare2brik(opj(path_afni_preprocessed_files, 'corr_brain+tlrc.BRIK'),'corr_brain+tlrc.BRIK') # TRUE



# create fitts dataset from all_runs and errts
calc = afni.Calc() # This program does voxel-by-voxel arithmetic on 3D datasets.
calc.inputs.in_file_a = 'all_runs.{}+tlrc.BRIK'.format(sub)
calc.inputs.in_file_b = 'errts.{}+tlrc.BRIK'.format(sub)
calc.inputs.expr = 'a-b'
calc.inputs.out_file = 'fitts.{}+tlrc.BRIK'.format(sub)
calc.cmdline
# should be:
# 3dcalc -a all_runs.$subj+tlrc -b errts.${subj}+tlrc -expr a-b            \
#        -prefix fitts.$subj
res = calc.run()
compare2brik(opj(path_afni_preprocessed_files, 'fitts.{}+tlrc.BRIK'.format(sub)),'fitts.{}+tlrc.BRIK'.format(sub)) # TRUE


# create ideal files for fixed response stim types
cat1d = afni.Cat()
# 1dcat takes as input one or more 1D files, and writes out a 1D file 
# containing the side-by-side concatenation of all or a subset of the columns from the input files.
cat1d.inputs.in_files = ["X.nocensor.xmat.1D"]
cat1d.inputs.sel = "'[12]'"
cat1d.inputs.out_file = 'ideal_vis.1D'
cat1d.cmdline
# should be
# 1dcat X.nocensor.xmat.1D'[12]' > ideal_vis.1D
res = cat1d.run()  
compare2text(3, opj(path_afni_preprocessed_files, 'ideal_vis.1D'),'ideal_vis.1D') # TRUE


cat1d = afni.Cat()
# 1dcat takes as input one or more 1D files, and writes out a 1D file 
# containing the side-by-side concatenation of all or a subset of the columns from the input files.
cat1d.inputs.in_files = ["X.nocensor.xmat.1D"]
cat1d.inputs.sel = "'[13]'"
cat1d.inputs.out_file = 'ideal_aud.1D'
cat1d.cmdline
# should be
# 1dcat X.nocensor.xmat.1D'[13]' > ideal_aud.1D
res = cat1d.run()  
compare2text(3, opj(path_afni_preprocessed_files, 'ideal_aud.1D'),'ideal_aud.1D') # TRUE


# --------------------------------------------------

# extract non-baseline regressors from the X-matrix,
# then compute their sum
odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
odt.inputs.in_file = 'X.nocensor.xmat.1D'
odt.inputs.args = "-write_xstim X.stim.xmat.1D"
odt.cmdline
# should be:
# 1d_tool.py -infile X.nocensor.xmat.1D -write_xstim X.stim.xmat.1D
res = odt.run() 
compare2text(3, opj(path_afni_preprocessed_files, 'X.stim.xmat.1D'),'X.stim.xmat.1D') # TRUE


tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_file = 'X.stim.xmat.1D'
tstat.inputs.out_file = "sum_ideal.1D"
tstat.args = "-sum"
tstat.cmdline
# should be
# 3dTstat -sum -prefix sum_ideal.1D X.stim.xmat.1D
res = tstat.run()
compare2text(3, opj(path_afni_preprocessed_files, 'sum_ideal.1D'),'sum_ideal.1D') # TRUE


# ============================ blur estimation =============================

################################
####### BLOCK EXPLANATION ######
"""
1. compute blur estimates
1.1. for each run
1.1.1 Calculate ratio of variance of first differences to data variance
    restrict to uncensored TRs, per run
    ACF option : 'ACF' stands for (spatial) AutoCorrelation
        Function, and it is estimated by calculating moments of differences out to
        a larger radius than before.
    from epicts.FT
    save in blur.epits.1D
1.2. compute average FWHM blur (from every other row) and append in blur_est.{}.1D
2.1 for each run 
2.1.1 Calculate ratio of variance of first differences to data variance
    restrict to uncensored TRs, per run
    from errts.FT+tlrc
    save in blur.errts.1D
3. save all results average epits FWHM blurs and average errts FWHM blurs 
    in  blur_est.{}.1D

"""
##################################
##################################

# compute blur estimates
from pathlib import Path
Path('blur_est.{}.1D'.format(sub)).touch() # start with empty file

# create directory for ACF curve files
os.mkdir('files_ACF')

# -- estimate blur for each run in epits --
Path('blur.epits.1D').touch()


# restrict to uncensored TRs, per run
""" ERROR can't get odt output that first into fwhm
for run in runs:
    
    odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
    odt.inputs.in_file = 'X.xmat.1D'
    odt.inputs.show_trs_uncensored = 'encoded'
    odt.inputs.show_trs_run = run
    odt.inputs.args = "-write_xstim X.stim.xmat.1D"
    odt.cmdline
    # should be:
    # set trs = `1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded \
    #                       -show_trs_run $run`
    trs = odt.run()
    
    if trs == "":
        fwhm = afni.FWHMx() # Unlike the older 3dFWHM, this program computes FWHMs for all sub-bricks in the input dataset, each one separately.
        fwhm.inputs.in_file = 'all_runs.{}+tlrc"[trs]"'.format(sub)
        fwhm.inputs.detrend = True
        fwhm.inputs.mask = "mask_epi_anat.{}+tlrc".format(sub)
        fwhm.inputs.ACF = 'files_ACF/out.3dFWHMx.ACF.epits.r{}.1D'.format(run)
        fwhm.inputs.out_file = 'blur.epits.1D'
        fwhm.cmdline
        # should be
        # 3dFWHMx -detrend -mask mask_epi_anat.$subj+tlrc                      \
        # -ACF files_ACF/out.3dFWHMx.ACF.epits.r$run.1D                \
        # all_runs.$subj+tlrc"[$trs]" >> blur.epits.1D
        res = fwhm.run()  # doctest: +SKIP
"""
# restrict to uncensored TRs, per run
trs = !1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded -show_trs_run 01
assert trs != [""]
!3dFWHMx -detrend -mask mask_epi_anat.FT+tlrc \
        -ACF files_ACF/out.3dFWHMx.ACF.epits.r01.1D \
        all_runs.FT+tlrc"[0..40,45..149]" >> blur.epits.1D

trs = !1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded -show_trs_run 02
assert trs != [""]
!3dFWHMx -detrend -mask mask_epi_anat.FT+tlrc \
        -ACF files_ACF/out.3dFWHMx.ACF.epits.r02.1D \
        all_runs.FT+tlrc"[150..264,267..299]" >> blur.epits.1D
trs = !1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded -show_trs_run 03
assert trs != [""]
!3dFWHMx -detrend -mask mask_epi_anat.FT+tlrc \
        -ACF files_ACF/out.3dFWHMx.ACF.epits.r03.1D \
        all_runs.FT+tlrc"[300..449]" >> blur.epits.1D
compare2text(3, opj(path_afni_preprocessed_files, 'files_ACF/out.3dFWHMx.ACF.epits.r01.1D'),'files_ACF/out.3dFWHMx.ACF.epits.r01.1D') # TRUE
compare2text(3, opj(path_afni_preprocessed_files, 'files_ACF/out.3dFWHMx.ACF.epits.r02.1D'),'files_ACF/out.3dFWHMx.ACF.epits.r02.1D') # TRUE
compare2text(3, opj(path_afni_preprocessed_files, 'files_ACF/out.3dFWHMx.ACF.epits.r03.1D'),'files_ACF/out.3dFWHMx.ACF.epits.r03.1D') # TRUE
compare2text(3, opj(path_afni_preprocessed_files, 'blur.epits.1D'),'blur.epits.1D') # TRUE



# compute average FWHM blur (from every other row) and append
"""ERROR with input file (must be transposed)
tstat = afni.TStat() # Compute voxel-wise statistics
tstat.inputs.in_files = "blur.epits.1D'{0..$(2)}'\'`"
tstat.args = '-mean'
tstat.cmdline
# should be
# set blurs = ( `3dTstat -mean -prefix - blur.epits.1D'{0..$(2)}'\'` )
blurs = tstat.run()
"""
# compute average FWHM blur (from every other row) and append
blurs = !3dTstat -mean -prefix - blur.epits.1D'{0..$(2)}'\' # gives 0 0 0 0
print("average epits FWHM blurs: {}".format(blurs))
with open("blur_est.{}.1D".format(sub), "a") as f:
  print("{} {} {} {}    # epits FWHM blur estimates".format(blurs[-4], blurs[-3],blurs[-2], blurs[-1]), file=f)
# compute average ACF blur (from every other row) and append
blurs = !3dTstat -mean -prefix - blur.epits.1D'{1..$(2)}'\'
print("average epits ACF blurs: {}".format(blurs))
with open("blur_est.{}.1D".format(sub), "a") as f:
  print("{} {} {} {}    # epits ACF blur estimates".format(blurs[-4], blurs[-3],blurs[-2], blurs[-1]), file=f)


# -- estimate blur for each run in errts --
Path('blur.errts.1D').touch()


# restrict to uncensored TRs, per run
""" ERROR can't get odt output that first into fwhm
for run in runs:
    odt = afni.OneDToolPy() # This program is meant to read/manipulate/write/diagnose 1D datasets.
    odt.inputs.in_file = 'X.xmat.1D'
    odt.show_trs_uncensored = 'encoded'
    odt.show_trs_run = run
    odt.inputs.write_xstim = 'X.stim.xmat.1D'
    odt.cmdline
    # should be:
    # set trs = `1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded \
    #                   -show_trs_run $run`
    trs = odt.run()

    if trs == "":
        fwhm = afni.FWHMx() # Unlike the older 3dFWHM, this program computes FWHMs for all sub-bricks in the input dataset, each one separately.
        fwhm.inputs.in_file = 'errts.{}+tlrc"[trs]"'.format(sub)
        fwhm.inputs.detrend = True
        fwhm.inputs.mask = "mask_epi_anat.{}+tlrc".format(sub)
        fwhm.inputs.ACF = 'files_ACF/out.3dFWHMx.ACF.errts.r{}.1D'.format(run)
        fwhm.inputs.out_file = 'blur.errts.1D'
        fwhm.cmdline
        # should be
        # 3dFWHMx -detrend -mask mask_epi_anat.$subj+tlrc                      \
        #         -ACF files_ACF/out.3dFWHMx.ACF.errts.r$run.1D                \
        #         errts.${subj}+tlrc"[$trs]" >> blur.errts.1D
        res = fwhm.run()  # doctest: +SKIP
"""
# restrict to uncensored TRs, per run
trs = !1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded -show_trs_run 01
assert trs != [""]
!3dFWHMx -detrend -mask mask_epi_anat.FT+tlrc \
        -ACF files_ACF/out.3dFWHMx.ACF.errts.r01.1D \
        errts.FT+tlrc"[0..40,45..149]" >> blur.errts.1D

trs = !1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded -show_trs_run 02
assert trs != [""]
!3dFWHMx -detrend -mask mask_epi_anat.FT+tlrc \
        -ACF files_ACF/out.3dFWHMx.ACF.errts.r02.1D \
        errts.FT+tlrc"[150..264,267..299]" >> blur.errts.1D
trs = !1d_tool.py -infile X.xmat.1D -show_trs_uncensored encoded -show_trs_run 03
assert trs != [""]
!3dFWHMx -detrend -mask mask_epi_anat.FT+tlrc \
        -ACF files_ACF/out.3dFWHMx.ACF.errts.r03.1D \
        errts.FT+tlrc"[300..449]" >> blur.errts.1D
compare2text(3, opj(path_afni_preprocessed_files, 'files_ACF/out.3dFWHMx.ACF.errts.r01.1D'),'files_ACF/out.3dFWHMx.ACF.errts.r01.1D') # TRUE
compare2text(3, opj(path_afni_preprocessed_files, 'files_ACF/out.3dFWHMx.ACF.errts.r02.1D'),'files_ACF/out.3dFWHMx.ACF.errts.r02.1D') # TRUE
compare2text(3, opj(path_afni_preprocessed_files, 'files_ACF/out.3dFWHMx.ACF.errts.r03.1D'),'files_ACF/out.3dFWHMx.ACF.errts.r03.1D') # TRUE
compare2text(3, opj(path_afni_preprocessed_files, 'blur.errts.1D'),'blur.errts.1D') # TRUE


# compute average FWHM blur (from every other row) and append
blurs = !3dTstat -mean -prefix - blur.errts.1D'{0..$(2)}'\' # gives 0 0 0 0
print("average errts FWHM blurs: {}".format(blurs))
with open("blur_est.{}.1D".format(sub), "a") as f:
  print("{} {} {} {}    # errts FWHM blur estimates".format(blurs[-4], blurs[-3],blurs[-2], blurs[-1]), file=f)
# compute average ACF blur (from every other row) and append
blurs = !3dTstat -mean -prefix - blur.epits.1D'{1..$(2)}'\'
print("average errts ACF blurs: {}".format(blurs))
with open("blur_est.{}.1D".format(sub), "a") as f:
  print("{} {} {} {}    # errts ACF blur estimates".format(blurs[-4], blurs[-3],blurs[-2], blurs[-1]), file=f)

# blur_est file doesn't exist in afni tuto with afni
compare2text(3, opj(path_afni_preprocessed_files, 'blur_est.FT.1D'),'blur_est.FT.1D') # TRUE



# ========================= auto block: QC_review ==========================
################################
####### BLOCK EXPLANATION ######
"""
1. generate quality control review scripts and HTML report
2. remove rm.* files

"""
##################################
##################################
# generate quality control review scripts and HTML report

# generate a review script for the unprocessed EPI data
"""missing
gen_epi_review.py -script @epi_review.$subj \
    -dsets pb00.$subj.r*.tcat+orig.HEAD
"""
!gen_epi_review.py -script @epi_review.FT \
    -dsets pb00.$subj.r*.tcat+orig.HEAD
compare2text(3, opj(path_afni_preprocessed_files, '@epi_review.FT'),'@epi_review.FT') # TRUE

# -------------------------------------------------
# generate scripts to review single subject results
# (try with defaults, but do not allow bad exit status)

"""missing thus copy
# write AP uvars into a simple txt file
cat << EOF > out.ap_uvars.txt
  mot_limit       : 0.3
  out_limit       : 0.05
  copy_anat       : FT_anat+orig.HEAD
  mask_dset       : mask_epi_anat.$subj+tlrc.HEAD
  tlrc_base       : TT_N27+tlrc.HEAD
  ss_review_dset  : out.ss_review.$subj.txt
  vlines_tcat_dir : vlines.pb00.tcat
EOF
"""
shutil.copyfile(opj(path_afni_preprocessed_files, "out.ap_uvars.txt"), "out.ap_uvars.txt")
compare2text(3, opj(path_afni_preprocessed_files, "out.ap_uvars.txt"),"out.ap_uvars.txt") # TRUE


"""missing thus copy
# and convert the txt format to JSON
cat out.ap_uvars.txt | afni_python_wrapper.py -eval "data_file_to_json()" \
  > out.ap_uvars.json
"""
shutil.copyfile(opj(path_afni_preprocessed_files, "out.ap_uvars.json"), "out.ap_uvars.json")
compare2text(3, opj(path_afni_preprocessed_files, "out.ap_uvars.json"),"out.ap_uvars.json") # TRUE

# initialize gen_ss_review_scripts.py with out.ap_uvars.json
!gen_ss_review_scripts.py -exit0        \
    -init_uvars_json out.ap_uvars.json \
    -write_uvars_json out.ss_review_uvars.json

# ========================== auto block: finalize ==========================

"""missing
# remove temporary files
\rm -f rm.*
"""
from glob import glob
rm_files_list = glob("rm.*")
for rm_file in rm_files_list:
    os.remove(rm_file) 

"""missing
# if the basic subject review script is here, run it
# (want this to be the last text output)
if ( -e @ss_review_basic ) then
    ./@ss_review_basic |& tee out.ss_review.$subj.txt

    # generate html ss review pages
    # (akin to static images from running @ss_review_driver)
    apqc_make_tcsh.py -review_style pythonic -subj_dir . \
        -uvar_json out.ss_review_uvars.json
    tcsh @ss_review_html |& tee out.review_html
    apqc_make_html.py -qc_dir QC_$subj

    echo "\nconsider running: \n"
    echo "   afni_open -b $subj.results/QC_$subj/index.html"
    echo ""
endif

# return to parent directory (just in case...)
cd ..

echo "execution finished: `date`"

"""


# ==========================================================================
# script generated by the command:
#
# afni_proc.py -subj_id FT -blocks tshift align tlrc volreg blur mask scale   \
#     regress -radial_correlate_blocks tcat volreg -copy_anat FT/FT_anat+orig \
#     -dsets FT/FT_epi_r1+orig.HEAD FT/FT_epi_r2+orig.HEAD                    \
#     FT/FT_epi_r3+orig.HEAD -tcat_remove_first_trs 2 -align_unifize_epi      \
#     local -align_opts_aea -cost lpc+ZZ -giant_move -check_flip              \
#     -volreg_align_to MIN_OUTLIER -volreg_align_e2a -volreg_tlrc_warp        \
#     -volreg_compute_tsnr yes -blur_size 4.0 -mask_epi_anat yes              \
#     -regress_stim_times FT/AV1_vis.txt FT/AV2_aud.txt -regress_stim_labels  \
#     vis aud -regress_basis 'BLOCK(20,1)' -regress_censor_motion 0.3         \
#     -regress_censor_outliers 0.05 -regress_motion_per_run -regress_opts_3dD \
#     -jobs 2 -gltsym 'SYM: vis -aud' -glt_label 1 V-A -gltsym 'SYM: 0.5*vis  \
#     +0.5*aud' -glt_label 2 mean.VA -regress_compute_fitts                   \
#     -regress_make_ideal_sum sum_ideal.1D -regress_est_blur_epits            \
#     -regress_est_blur_errts -regress_run_clustsim no -html_review_style     \
#     pythonic -execute

















